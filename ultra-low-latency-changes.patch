From 35d9ff22bc1de449a313310725a952e95a1781d6 Mon Sep 17 00:00:00 2001
From: Nikunj <nikunj.patriwala@gmail.com>
Date: Thu, 14 Aug 2025 01:22:50 +0530
Subject: [PATCH 1/4] initial commit

---
 README.md | 1 -
 1 file changed, 1 deletion(-)

diff --git a/README.md b/README.md
index 5544b60..2c94e82 100644
--- a/README.md
+++ b/README.md
@@ -3,7 +3,6 @@
 A real-time speech-to-text application with live transcription capabilities using React frontend and FastAPI backend with Deepgram integration.
 
 ## 🚀 Features
-
 - **Real-time Speech Recognition**: Live audio transcription using Deepgram API
 - **WebSocket Communication**: Seamless real-time communication between frontend and backend
 - **Audio Processing**: Raw PCM audio capture and processing
-- 
2.39.5 (Apple Git-154)


From 6c5f4c03c6600a9e73b6f051996c336c6666a951 Mon Sep 17 00:00:00 2001
From: Nikunj <nikunj.patriwala@gmail.com>
Date: Thu, 14 Aug 2025 01:27:50 +0530
Subject: [PATCH 2/4] Added fix for the transcript format

---
 spokenly-backend/main.py          | 802 ++++++++----------------------
 spokenly-backend/requirements.txt |   1 +
 spokenly-frontend/src/App.js      | 741 ++++++++-------------------
 3 files changed, 414 insertions(+), 1130 deletions(-)

diff --git a/spokenly-backend/main.py b/spokenly-backend/main.py
index 09aa3b9..534b85c 100644
--- a/spokenly-backend/main.py
+++ b/spokenly-backend/main.py
@@ -4,28 +4,45 @@ import logging
 import os
 import time
 import numpy as np
-from typing import Dict, Optional, Set, List, Tuple
-from contextlib import asynccontextmanager
+from typing import Dict, Optional, Set, List
 
-import websockets
+# Deepgram SDK imports
+from deepgram import (
+    DeepgramClient,
+    DeepgramClientOptions,
+    LiveTranscriptionEvents,
+    LiveOptions,
+    DeepgramError,
+)
+# Corrected imports for response types for deepgram-sdk v4.x
+from deepgram.clients.listen import (
+    LiveResultResponse,
+    UtteranceEndResponse,
+)
+
+from deepgram.clients.listen.v1.websocket import (
+    MetadataResponse
+)
+
+# FastAPI imports
 from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException
 from fastapi.middleware.cors import CORSMiddleware
 from fastapi.responses import JSONResponse
 from pydantic_settings import BaseSettings
 from pydantic import Field
-from starlette.websockets import WebSocketDisconnect
+import logging
+
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.DEBUG)  # Set the lowest level to capture all messages.
 
 # Audio processing imports
 try:
     import webrtcvad
-    import librosa
-    from scipy import signal
     from scipy.signal import butter, filtfilt
     AUDIO_PROCESSING_AVAILABLE = True
 except ImportError:
     AUDIO_PROCESSING_AVAILABLE = False
-    logger = logging.getLogger(__name__)
-    logger.warning("Audio processing libraries not available. Install numpy, scipy, librosa, webrtcvad for enhanced audio processing.")
+    logging.warning("Audio processing libraries not available. Install numpy, scipy, webrtcvad for enhanced audio processing.")
 
 # Configure logging
 logging.basicConfig(
@@ -37,14 +54,11 @@ logger = logging.getLogger(__name__)
 class Settings(BaseSettings):
     """Application settings with environment variable support."""
     deepgram_api_key: str = Field(..., env="DEEPGRAM_API_KEY")
-    deepgram_url: str = Field("wss://api.deepgram.com/v1/listen", env="DEEPGRAM_URL")
     deepgram_language: str = Field("en-US", env="DEEPGRAM_LANGUAGE")
-    deepgram_model: str = Field("nova-2", env="DEEPGRAM_MODEL")  # Use Nova-2 for compatibility
-    deepgram_tier: str = Field("", env="DEEPGRAM_TIER")  # No tier parameter for compatibility
+    deepgram_model: str = Field("nova-2", env="DEEPGRAM_MODEL")
     max_connections_per_user: int = Field(50, env="MAX_CONNECTIONS_PER_USER")
     connection_timeout: int = Field(30, env="CONNECTION_TIMEOUT")
-    retry_attempts: int = Field(3, env="RETRY_ATTEMPTS")
-    retry_delay: float = Field(1.0, env="RETRY_DELAY")
+    
     # Audio processing settings
     sample_rate: int = Field(16000, env="SAMPLE_RATE")
     enable_vad: bool = Field(False, env="ENABLE_VAD")
@@ -54,18 +68,30 @@ class Settings(BaseSettings):
     
     class Config:
         env_file = ".env"
-        extra = "ignore"  # Ignore extra fields from .env file
+        extra = "ignore"
 
 settings = Settings()
 
+# Initialize Deepgram Client
+# The SDK will automatically handle retries and connection management.
+try:
+    config = DeepgramClientOptions(
+        verbose=logging.WARNING,  # Change to logging.DEBUG for more verbose output
+        options={"keepalive": "true"}
+    )
+    deepgram: DeepgramClient = DeepgramClient(settings.deepgram_api_key, config)
+except Exception as e:
+    logger.error(f"Could not create Deepgram client: {e}")
+    raise
+
 class ConnectionManager:
     """Manages WebSocket connections and provides connection tracking."""
     
     def __init__(self):
         self.active_connections: Dict[str, Set[WebSocket]] = {}
         self.connection_metadata: Dict[WebSocket, Dict] = {}
-        self.session_transcripts: Dict[str, list] = {}  # Store transcripts per session
-    
+        self.session_transcripts: Dict[str, list] = {}
+
     async def connect(self, websocket: WebSocket, session_id: str = "anonymous", role: str = "recorder"):
         """Accept a new WebSocket connection and track it."""
         await websocket.accept()
@@ -73,7 +99,6 @@ class ConnectionManager:
         if session_id not in self.active_connections:
             self.active_connections[session_id] = set()
         
-        # Check connection limits (allow more connections per session)
         if len(self.active_connections[session_id]) >= settings.max_connections_per_user * 2:
             await websocket.close(code=1008, reason="Too many connections")
             return False
@@ -83,13 +108,11 @@ class ConnectionManager:
             "session_id": session_id,
             "role": role,
             "connected_at": time.time(),
-            "messages_sent": 0,
-            "messages_received": 0
         }
         
         logger.info(f"New {role} connection for session {session_id}. Total connections: {len(self.active_connections[session_id])}")
         return True
-    
+
     def disconnect(self, websocket: WebSocket):
         """Remove a WebSocket connection from tracking."""
         metadata = self.connection_metadata.get(websocket)
@@ -103,45 +126,37 @@ class ConnectionManager:
             
             del self.connection_metadata[websocket]
             logger.info(f"Connection closed for session {session_id} (role: {role})")
-    
+
     async def send_personal_message(self, message: dict, websocket: WebSocket):
         """Send a message to a specific WebSocket connection."""
         try:
             await websocket.send_json(message)
-            self.connection_metadata[websocket]["messages_sent"] += 1
         except Exception as e:
             logger.error(f"Failed to send message: {e}")
             self.disconnect(websocket)
-    
+
     async def broadcast_to_session(self, message: dict, session_id: str, exclude_websocket: WebSocket = None):
         """Send a message to all WebSocket connections for a specific session."""
-        logger.info(f"Broadcasting to session {session_id}: {message}")
-        
-        if session_id in self.active_connections:
-            connection_count = len(self.active_connections[session_id])
-            logger.info(f"Found {connection_count} connections for session {session_id}")
-            
-            disconnected_websockets = []
-            for websocket in self.active_connections[session_id]:
-                # Skip the excluded websocket (usually the sender)
-                if exclude_websocket and websocket == exclude_websocket:
-                    logger.info(f"Skipping sender websocket for session {session_id}")
-                    continue
-                    
-                try:
-                    await websocket.send_json(message)
-                    self.connection_metadata[websocket]["messages_sent"] += 1
-                    logger.info(f"Successfully sent message to viewer for session {session_id}")
-                except Exception as e:
-                    logger.error(f"Failed to broadcast message to session {session_id}: {e}")
-                    disconnected_websockets.append(websocket)
-            
-            # Clean up disconnected websockets
-            for websocket in disconnected_websockets:
-                self.disconnect(websocket)
-        else:
+        if session_id not in self.active_connections:
             logger.warning(f"No connections found for session {session_id}")
-    
+            return
+            
+        disconnected_websockets = []
+        for websocket in self.active_connections[session_id]:
+            logger.info( "Found WebSocket:" )
+            if exclude_websocket and websocket == exclude_websocket:
+                continue
+                
+            try:
+                logger.info( f"Sending message : {json.dumps(message)}" )
+                await websocket.send_json(message)
+            except Exception as e:
+                logger.error(f"Failed to broadcast message to session {session_id}: {e}")
+                disconnected_websockets.append(websocket)
+        
+        for websocket in disconnected_websockets:
+            self.disconnect(websocket)
+
     async def send_session_history(self, websocket: WebSocket, session_id: str):
         """Send all historical transcripts for a session to a new viewer."""
         if session_id in self.session_transcripts:
@@ -153,562 +168,166 @@ class ConnectionManager:
                     "session_id": session_id
                 }
                 await self.send_personal_message(history_message, websocket)
-                logger.info(f"Sent {len(transcripts)} historical transcripts to viewer for session {session_id}")
-    
+
     def add_transcript_to_session(self, session_id: str, transcript: dict):
         """Add a transcript to the session history."""
         if session_id not in self.session_transcripts:
             self.session_transcripts[session_id] = []
         
         self.session_transcripts[session_id].append(transcript)
-        logger.info(f"Added transcript to session {session_id} history. Total: {len(self.session_transcripts[session_id])}")
-    
+
     def get_connection_stats(self) -> Dict:
         """Get connection statistics for monitoring."""
-        total_connections = sum(len(connections) for connections in self.active_connections.values())
         return {
-            "total_connections": total_connections,
+            "total_connections": sum(len(conns) for conns in self.active_connections.values()),
             "sessions_active": len(self.active_connections),
-            "connections_by_session": {session: len(connections) for session, connections in self.active_connections.items()},
-            "sessions_with_transcripts": len(self.session_transcripts)
         }
 
-class DeepgramService:
-    """Handles communication with Deepgram's WebSocket API."""
+class AudioProcessor:
+    """Handles audio processing and transcription logic using Deepgram SDK."""
     
     def __init__(self):
-        self.connection_attempts = 0
-    
-    @asynccontextmanager
-    async def get_connection(self, language: str = None):
-        """Context manager for Deepgram WebSocket connection with retry logic."""
-        connection = None
-        try:
-            for attempt in range(settings.retry_attempts):
-                try:
-                    # Get optimal parameters for the detected language
-                    params = self._get_deepgram_params(language or settings.deepgram_language)
-                    
-                    # Build query string from parameters
-                    query_params = "&".join([f"{k}={v}" for k, v in params.items() if v])
-                    uri = f"{settings.deepgram_url}?{query_params}"
-                    
-                    headers = {"Authorization": f"Token {settings.deepgram_api_key}"}
-                    
-                    logger.info(f"Connecting to Deepgram with URI: {uri}")
-                    connection = await websockets.connect(
-                        uri, 
-                        extra_headers=headers,
-                        ping_interval=20,
-                        ping_timeout=10
-                    )
-                    self.connection_attempts = 0
-                    logger.info(f"Successfully connected to Deepgram with language: {params.get('language', 'unknown')}")
-                    yield connection
-                    break
-                    
-                except Exception as e:
-                    self.connection_attempts += 1
-                    logger.error(f"Deepgram connection attempt {attempt + 1} failed: {e}")
-                    
-                    if attempt < settings.retry_attempts - 1:
-                        await asyncio.sleep(settings.retry_delay * (2 ** attempt))  # Exponential backoff
-                    else:
-                        logger.error("All Deepgram connection attempts failed")
-                        raise
-        finally:
-            if connection:
-                await connection.close()
-    
-    def _get_deepgram_params(self, detected_language: str) -> Dict[str, str]:
-        """Get optimal Deepgram parameters for the detected language."""
-        base_params = {
-            "model": settings.deepgram_model,
-            "language": detected_language,
-            "punctuate": "true",
-            "interim_results": "true",
-            "smart_format": "true",
-            "encoding": "linear16",
-            "channels": "1",
-            "sample_rate": str(settings.sample_rate)
-        }
-        
-        # Only add tier if it's not empty
-        if settings.deepgram_tier:
-            base_params["tier"] = settings.deepgram_tier
-        
-        return base_params
+        # This class is kept for structure, but processing is currently bypassed.
+        pass
 
-class AudioProcessor:
-    """Handles audio processing and transcription logic with enhanced quality."""
-    
-    def __init__(self, deepgram_service: DeepgramService):
-        self.deepgram_service = deepgram_service
-        self.transcript_buffer = {}
-        
-        # Initialize VAD if available
-        if AUDIO_PROCESSING_AVAILABLE and settings.enable_vad:
-            try:
-                self.vad = webrtcvad.Vad(settings.vad_aggressiveness)
-                self.vad_enabled = True
-            except Exception as e:
-                logger.warning(f"Failed to initialize VAD: {e}")
-                self.vad_enabled = False
-        else:
-            self.vad_enabled = False
-        
-        # Audio processing parameters
-        self.frame_duration = 30  # ms
-        self.frame_size = int(settings.sample_rate * self.frame_duration / 1000)
-        self.audio_buffer = []
-        self.silence_threshold = 0.01
-        self.noise_reduction_strength = 0.1
-        
-        # Dialect and language support
-        self.supported_languages = {
-            "en-US": ["en-US", "en-GB", "en-AU", "en-IN", "en-NZ"],
-            "es-ES": ["es-ES", "es-MX", "es-AR", "es-CO", "es-PE"],
-            "fr-FR": ["fr-FR", "fr-CA", "fr-BE", "fr-CH"],
-            "de-DE": ["de-DE", "de-AT", "de-CH"],
-            "hi-IN": ["hi-IN", "en-IN"],  # Hindi with English code-switching
-            "ar-SA": ["ar-SA", "ar-EG", "ar-MA", "ar-DZ"],
-            "zh-CN": ["zh-CN", "zh-TW", "zh-HK"],
-            "ja-JP": ["ja-JP"],
-            "ko-KR": ["ko-KR"],
-            "pt-BR": ["pt-BR", "pt-PT"],
-            "ru-RU": ["ru-RU"],
-            "it-IT": ["it-IT", "it-CH"],
-            "nl-NL": ["nl-NL", "nl-BE"],
-            "pl-PL": ["pl-PL"],
-            "tr-TR": ["tr-TR"],
-            "sv-SE": ["sv-SE"],
-            "da-DK": ["da-DK"],
-            "no-NO": ["no-NO"],
-            "fi-FI": ["fi-FI"],
-            "he-IL": ["he-IL"],
-            "th-TH": ["th-TH"],
-            "vi-VN": ["vi-VN"],
-            "id-ID": ["id-ID"],
-            "ms-MY": ["ms-MY"],
-            "tl-PH": ["tl-PH"],
-            "bn-IN": ["bn-IN"],
-            "ta-IN": ["ta-IN"],
-            "te-IN": ["te-IN"],
-            "kn-IN": ["kn-IN"],
-            "ml-IN": ["ml-IN"],
-            "gu-IN": ["gu-IN"],
-            "pa-IN": ["pa-IN"],
-            "or-IN": ["or-IN"],
-            "as-IN": ["as-IN"],
-            "ne-NP": ["ne-NP"],
-            "si-LK": ["si-LK"],
-            "my-MM": ["my-MM"],
-            "km-KH": ["km-KH"],
-            "lo-LA": ["lo-LA"],
-            "mn-MN": ["mn-MN"],
-            "ka-GE": ["ka-GE"],
-            "hy-AM": ["hy-AM"],
-            "az-AZ": ["az-AZ"],
-            "kk-KZ": ["kk-KZ"],
-            "ky-KG": ["ky-KG"],
-            "uz-UZ": ["uz-UZ"],
-            "tk-TM": ["tk-TM"],
-            "tg-TJ": ["tg-TJ"],
-            "fa-IR": ["fa-IR"],
-            "ps-AF": ["ps-AF"],
-            "ur-PK": ["ur-PK"],
-            "sd-PK": ["sd-PK"],
-            "bn-BD": ["bn-BD"],
-            "si-LK": ["si-LK"],
-            "my-MM": ["my-MM"],
-            "km-KH": ["km-KH"],
-            "lo-LA": ["lo-LA"],
-            "mn-MN": ["mn-MN"],
-            "ka-GE": ["ka-GE"],
-            "hy-AM": ["hy-AM"],
-            "az-AZ": ["az-AZ"],
-            "kk-KZ": ["kk-KZ"],
-            "ky-KG": ["ky-KG"],
-            "uz-UZ": ["uz-UZ"],
-            "tk-TM": ["tk-TM"],
-            "tg-TJ": ["tg-TJ"],
-            "fa-IR": ["fa-IR"],
-            "ps-AF": ["ps-AF"],
-            "ur-PK": ["ur-PK"],
-            "sd-PK": ["sd-PK"],
-            "bn-BD": ["bn-BD"]
-        }
-    
     async def process_audio_stream(self, websocket: WebSocket, session_id: str):
-        """Process incoming audio stream and return transcriptions."""
-        # Detect language from initial audio samples
-        detected_language = settings.deepgram_language
-        language_detection_samples = []
+        """Process incoming audio stream using Deepgram SDK."""
         
-        # Collect some audio samples for language detection
+        dg_connection = None
         try:
-            for _ in range(10):  # Collect 10 audio frames for detection
-                try:
-                    audio_data = await asyncio.wait_for(websocket.receive_bytes(), timeout=0.1)
-                    language_detection_samples.append(audio_data)
-                    if len(language_detection_samples) >= 5:  # Use first 5 frames
-                        break
-                except asyncio.TimeoutError:
-                    break
-        except Exception as e:
-            logger.warning(f"Could not collect language detection samples: {e}")
-        
-        # Detect language if we have samples
-        if language_detection_samples:
-            try:
-                combined_audio = b''.join(language_detection_samples)
-                detected_language = self.detect_language_and_dialect(combined_audio)
-                logger.info(f"Detected language: {detected_language}")
-            except Exception as e:
-                logger.warning(f"Language detection failed: {e}")
-        
-        async with self.deepgram_service.get_connection(detected_language) as dg_ws:
-            # Create tasks for bidirectional communication
-            send_task = asyncio.create_task(self._send_audio_to_deepgram(websocket, dg_ws, language_detection_samples))
-            receive_task = asyncio.create_task(self._receive_transcriptions(websocket, dg_ws, session_id))
-            
-            try:
-                # Wait for either task to complete (or fail)
-                done, pending = await asyncio.wait(
-                    [send_task, receive_task],
-                    return_when=asyncio.FIRST_COMPLETED
-                )
+            # STEP 1: Create a Deepgram LiveTranscription connection with advanced options
+            options = self._get_deepgram_options(settings.deepgram_language)
+            dg_connection = deepgram.listen.asyncwebsocket.v("1")
+
+            # STEP 2: Define event handlers
+            async def on_open(cls, open, **kwargs):
+                logger.info(f"Deepgram connection opened for session {session_id}.")
+
+            async def on_message(cls,result: LiveResultResponse, **kwargs):
+                logger.info(f" on_message ".center( 80 , "=" ) )
                 
-                # Cancel remaining tasks
-                for task in pending:
-                    task.cancel()
-                    try:
-                        await task
-                    except asyncio.CancelledError:
-                        pass
-                        
-            except Exception as e:
-                logger.error(f"Error in audio processing: {e}")
-                raise
-    
-    async def _send_audio_to_deepgram(self, websocket: WebSocket, dg_ws, language_detection_samples: List[bytes] = None):
-        """Send audio data from client to Deepgram with enhanced processing."""
-        try:
-            # First, send the language detection samples if we have them
-            if language_detection_samples:
-                for audio_data in language_detection_samples:
-                    processed_audio = self.process_audio_frame(audio_data)
-                    if processed_audio:  # Only send if we have audio after processing
-                        await dg_ws.send(processed_audio)
-            
-            # Then process the ongoing audio stream
+                logger.info(f" Result Data ".center( 80 , "-" ) )
+                logger.info(result)
+                logger.info(f"-"*80)
+
+                transcript_data = self._extract_transcript(result)
+                
+                logger.info(f" Transcipt Data ".center( 80 , "~" ) )
+                logger.info(transcript_data)
+                logger.info(f"~"*80)
+
+                logger.info(f"=" * 80)
+
+                if transcript_data:
+                    if transcript_data["is_final"]:
+                        manager.add_transcript_to_session(session_id, transcript_data)
+                    await manager.broadcast_to_session(transcript_data, session_id)
+
+            async def on_metadata(cls, metadata: MetadataResponse, **kwargs):
+                logger.info(f" on_metadata ".center( 80 , "=" ) )
+                logger.info(metadata)
+                logger.info(f"=" * 80)
+                logger.info(f"Deepgram metadata received for session {session_id}: {metadata}")
+
+            async def on_utterance_end(cls, utterance_end: UtteranceEndResponse, **kwargs):
+                logger.info(f" on_metadata ".center( 80 , "=" ) )
+                logger.info(utterance_end)
+                logger.info(f"=" * 80)
+                await manager.broadcast_to_session({"type": "utterance_end"}, session_id)
+
+            async def on_error(cls, error: DeepgramError, **kwargs):
+                logger.error(f"Deepgram error for session {session_id}: {error}")
+                await manager.broadcast_to_session({"type": "error", "message": str(error)}, session_id)
+
+            async def on_close(cls, close, **kwargs):
+                logger.info(f"Deepgram connection closed for session {session_id}.")
+
+            # STEP 3: Register event handlers
+            dg_connection.on(LiveTranscriptionEvents.Open, on_open)
+            dg_connection.on(LiveTranscriptionEvents.Transcript, on_message)
+            dg_connection.on(LiveTranscriptionEvents.Metadata, on_metadata)
+            dg_connection.on(LiveTranscriptionEvents.UtteranceEnd, on_utterance_end)
+            dg_connection.on(LiveTranscriptionEvents.Error, on_error)
+            dg_connection.on(LiveTranscriptionEvents.Close, on_close)
+
+            # STEP 4: Start the connection
+            if not await dg_connection.start(options):
+                logger.error(f"Failed to start Deepgram connection for session {session_id}")
+                return
+
+            # STEP 5: Process audio from the client WebSocket
             while True:
                 try:
-                    # Check if websocket is still connected before receiving
-                    if websocket.client_state.value == 3:  # WebSocketState.DISCONNECTED
-                        logger.info("WebSocket disconnected, stopping audio send")
-                        break
-                    
                     audio_data = await websocket.receive_bytes()
-                    logger.info(f"Received audio data: {len(audio_data)} bytes")
-                    
-                    # Process the audio frame for better quality
-                    processed_audio = self.process_audio_frame(audio_data)
-                    
-                    # Only send if we have audio after processing (VAD might remove silence)
-                    if processed_audio:
-                        # Debug: log audio data occasionally
-                        if len(processed_audio) > 0 and len(processed_audio) % 100 == 0:  # Log more frequently
-                            # Check if audio is all zeros (silence)
-                            audio_array = np.frombuffer(processed_audio, dtype=np.int16)
-                            max_amplitude = np.max(np.abs(audio_array))
-                            mean_amplitude = np.mean(np.abs(audio_array))
-                            logger.info(f"Audio debug - Size: {len(processed_audio)}, Max: {max_amplitude}, Mean: {mean_amplitude:.1f}, First 4 bytes: {processed_audio[:4].hex()}")
-                            
-                            # Check if audio is all zeros
-                            if max_amplitude == 0:
-                                logger.warning("⚠️ Audio is completely silent (all zeros)")
-                            elif max_amplitude < 100:
-                                logger.warning(f"⚠️ Audio amplitude too low: {max_amplitude} (should be > 100 for speech)")
-                            else:
-                                logger.info(f"✅ Audio has good amplitude: {max_amplitude}")
-                        
-                        logger.info(f"Sending {len(processed_audio)} bytes to Deepgram")
-                        await dg_ws.send(processed_audio)
-                    else:
-                        logger.info("No audio data to send after processing")
-                        
+                    await dg_connection.send(audio_data)
                 except WebSocketDisconnect:
-                    logger.info("Client disconnected during audio send")
+                    logger.info(f"Client disconnected for session {session_id}.")
                     break
-                except RuntimeError as e:
-                    if "disconnect message has been received" in str(e):
-                        logger.info("WebSocket disconnect detected, stopping audio send")
-                        break
-                    else:
-                        raise
-                
-        except Exception as e:
-            logger.error(f"Error sending audio to Deepgram: {e}")
-            raise
-    
-    async def _receive_transcriptions(self, websocket: WebSocket, dg_ws, session_id: str):
-        """Receive transcriptions from Deepgram and send to client."""
-        try:
-            logger.info(f"Starting to receive transcriptions from Deepgram for session {session_id}")
-            async for msg in dg_ws:
-                try:
-                    logger.info(f"Received message from Deepgram: {len(msg)} bytes")
-                    response = json.loads(msg)
-                    logger.info(f"Parsed Deepgram response: {response}")
-                    
-                    # Extract enhanced transcript from Deepgram response
-                    transcript_data = self._extract_transcript(response)
-                    if transcript_data:
-                        logger.info(f"Extracted transcript data: {transcript_data}")
-                        
-                        # Always send both interim and final transcripts to the frontend
-                        # Only add finalized transcripts to session history
-                        if transcript_data["is_final"]:
-                            manager.add_transcript_to_session(session_id, transcript_data)
-                            logger.info(f"Added final transcript to session history: {transcript_data['text']}")
-
-                        # Prepare transcript message for frontend
-                        transcript_message = {
-                            "type": "transcript",
-                            "text": transcript_data["text"],
-                            "speaker": transcript_data["speaker"],
-                            "is_final": transcript_data["is_final"],
-                            "confidence": transcript_data["confidence"],
-                            "detected_language": transcript_data["detected_language"],
-                            "topics": transcript_data["topics"],
-                            "utterances": transcript_data["utterances"],
-                            "timestamp": transcript_data["timestamp"]
-                        }
-
-                        # Debug: log every transcript sent to frontend
-                        logger.info(f"[LIVE TRANSCRIPT] Session {session_id} | Final: {transcript_data['is_final']} | Text: '{transcript_data['text']}' | Confidence: {transcript_data['confidence']}")
-
-                        try:
-                            # Send to the original websocket (recorder)
-                            await websocket.send_json(transcript_message)
-                            logger.info(f"✅ Successfully sent transcript to recorder for session {session_id}")
-                            
-                            # Broadcast to other connections for the same session (excluding the sender)
-                            await manager.broadcast_to_session(transcript_message, session_id, exclude_websocket=websocket)
-                        except Exception as e:
-                            logger.error(f"❌ Failed to send transcript to frontend: {e}")
-                    else:
-                        logger.info(f"No transcript data extracted from response: {response}")
-                        # Log the full response for debugging
-                        logger.debug(f"Full Deepgram response: {response}")
-                        
-                except json.JSONDecodeError as e:
-                    logger.error(f"Failed to parse Deepgram response: {e}")
-                    logger.error(f"Raw message: {msg}")
-                    
-        except websockets.exceptions.ConnectionClosed:
-            logger.info("Deepgram connection closed")
-        except Exception as e:
-            logger.error(f"Error receiving transcriptions: {e}")
-            raise
-    
-    def _extract_transcript(self, response: dict) -> Optional[dict]:
-        """Extract enhanced transcript information from Deepgram response."""
-        try:
-            # Log all response types for debugging
-            response_type = response.get("type")
-            logger.info(f"Processing Deepgram response type: {response_type}")
-            
-            # Only process Results responses, skip metadata
-            if response_type == "Metadata":
-                logger.info("Skipping metadata response")
-                return None
-            
-            # Handle Results responses
-            if response_type == "Results":
-                channel = response.get("channel", {})
-                alternatives = channel.get("alternatives", [])
-                
-                if not alternatives:
-                    logger.info("No alternatives found in response")
-                    return None
-                
-                alternative = alternatives[0]
-                transcript_text = alternative.get("transcript", "")
-                
-                # Allow empty transcripts for interim results
-                if not transcript_text and not response.get("is_final", False):
-                    logger.info("Empty interim transcript - allowing for continuation")
-                    return None
-                
-                if not transcript_text.strip():
-                    logger.info("Empty transcript text - skipping")
-                    return None
-            
-            # Extract speaker information if available
-            speaker = None
-            if "words" in alternative and alternative["words"]:
-                # Get speaker from first word
-                first_word = alternative["words"][0]
-                speaker = first_word.get("speaker", None)
-            
-            # Extract confidence score
-            confidence = alternative.get("confidence", 0.0)
-            
-            # Extract language if detected
-            detected_language = response.get("metadata", {}).get("language", None)
-            
-            # Extract topics if available
-            topics = response.get("metadata", {}).get("topics", [])
-            
-            # Extract utterances if available
-            utterances = response.get("metadata", {}).get("utterances", [])
-            
-            return {
-                "text": transcript_text,
-                "speaker": speaker,
-                "confidence": confidence,
-                "detected_language": detected_language,
-                "topics": topics,
-                "utterances": utterances,
-                "is_final": response.get("is_final", False),
-                "timestamp": time.time()
-            }
-            
+        
         except Exception as e:
-            logger.error(f"Error extracting transcript: {e}")
-            return None
-    
-    def get_user_transcript_history(self, user_id: str) -> list:
-        """Get transcript history for a specific user."""
-        return self.transcript_buffer.get(user_id, [])
-    
-    def process_audio_frame(self, audio_data: bytes) -> bytes:
-        """Process audio frame - now receiving raw linear16 data from frontend."""
-        if len(audio_data) == 0:
-            return None
+            logger.error(f"Error during audio processing for session {session_id}: {e}")
         
-        # Frontend now sends raw linear16 data, so we can pass it through directly
-        return audio_data
+        finally:
+            # STEP 6: Cleanly close the Deepgram connection
+            if dg_connection:
+                await dg_connection.finish()
+                logger.info(f"Finished Deepgram connection for session {session_id}.")
 
-    def _apply_noise_reduction(self, audio_array: np.ndarray) -> np.ndarray:
-        """Apply spectral noise reduction to audio."""
-        try:
-            # Convert to float for processing
-            audio_float = audio_array.astype(np.float32) / 32768.0
-            
-            # Apply spectral subtraction
-            # Calculate noise profile from first 0.5 seconds
-            noise_samples = int(0.5 * settings.sample_rate)
-            if len(audio_float) > noise_samples:
-                noise_profile = np.mean(np.abs(audio_float[:noise_samples]))
-                
-                # Apply spectral subtraction
-                fft = np.fft.fft(audio_float)
-                magnitude = np.abs(fft)
-                phase = np.angle(fft)
-                
-                # Subtract noise from magnitude
-                cleaned_magnitude = np.maximum(magnitude - noise_profile * self.noise_reduction_strength, 0)
-                
-                # Reconstruct signal
-                cleaned_fft = cleaned_magnitude * np.exp(1j * phase)
-                cleaned_audio = np.real(np.fft.ifft(cleaned_fft))
-                
-                # Convert back to int16
-                return (cleaned_audio * 32768.0).astype(np.int16)
-            
-            return audio_array
-            
-        except Exception as e:
-            logger.error(f"Error in noise reduction: {e}")
-            return audio_array
+    def _get_deepgram_options(self, language: str) -> LiveOptions:
+        """Create LiveOptions object with advanced settings."""
+        return LiveOptions(
+            model=settings.deepgram_model,
+            language=settings.deepgram_language,
+            punctuate=True,
+            interim_results=True,
+            smart_format=True,
+            encoding="linear16",
+            channels=1,
+            sample_rate=settings.sample_rate,
+            utterance_end_ms="5000",
+            vad_events=True,
+            endpointing=300,
+        )
 
-    def _apply_audio_enhancement(self, audio_array: np.ndarray) -> np.ndarray:
-        """Apply audio enhancement filters with robust error handling."""
-        try:
-            audio_float = audio_array.astype(np.float32) / 32768.0
-            nyquist = settings.sample_rate / 2
-            # High-pass filter
-            cutoff_hp = 80  # Hz
-            norm_hp = cutoff_hp / nyquist
-            if not (0 < norm_hp < 1):
-                raise ValueError("Digital filter critical frequencies must be 0 < Wn < 1 (highpass)")
-            b, a = butter(4, norm_hp, btype='high')
-            audio_float = filtfilt(b, a, audio_float)
-            # Low-pass filter
-            cutoff_lp = 4000  # Hz (reduced from 8000 to stay within valid range)
-            norm_lp = cutoff_lp / nyquist
-            if not (0 < norm_lp < 1):
-                raise ValueError("Digital filter critical frequencies must be 0 < Wn < 1 (lowpass)")
-            b, a = butter(4, norm_lp, btype='low')
-            audio_float = filtfilt(b, a, audio_float)
-            # Compression
-            threshold = 0.3
-            ratio = 2.0
-            audio_float = np.where(
-                np.abs(audio_float) > threshold,
-                np.sign(audio_float) * (threshold + (np.abs(audio_float) - threshold) / ratio),
-                audio_float
-            )
-            return (audio_float * 32768.0).astype(np.int16)
-        except Exception as e:
-            logger.error(f"Error in audio enhancement: {e}")
-            return audio_array
-    
-    def _apply_vad(self, audio_array: np.ndarray) -> np.ndarray:
-        """Apply Voice Activity Detection to remove silence."""
-        try:
-            # Process audio in frames
-            frame_size = self.frame_size
-            processed_frames = []
-            
-            for i in range(0, len(audio_array), frame_size):
-                frame = audio_array[i:i + frame_size]
-                if len(frame) == frame_size:
-                    # Check if frame contains speech
-                    is_speech = self.vad.is_speech(frame.tobytes(), settings.sample_rate)
-                    if is_speech:
-                        processed_frames.append(frame)
-                else:
-                    # Last frame, keep it
-                    processed_frames.append(frame)
+    def _extract_transcript(self, response: LiveResultResponse) -> Optional[dict]:
+        """Extract enhanced transcript information from Deepgram SDK response."""
+        # *** FIX: Only process 'Results' type messages ***
+        logger.info( "in _extract_transcript" )
+
+        if response.type != 'Results':
+            return None
             
-            if processed_frames:
-                return np.concatenate(processed_frames)
-            else:
-                return np.array([], dtype=np.int16)
-                
-        except Exception as e:
-            logger.error(f"Error in VAD: {e}")
-            return audio_array
-    
-    def detect_language_and_dialect(self, audio_data: bytes) -> str:
-        """Detect the most likely language and dialect from audio."""
-        # This is a simplified implementation
-        # In a production system, you might use a language detection service
-        # For now, we'll return the configured language
-        return settings.deepgram_language
-    
+        if not response.channel or not response.channel.alternatives:
+            return None
+
+        alternative = response.channel.alternatives[0]
+        transcript_text = alternative.transcript
+
+        if not transcript_text.strip():
+            return None
 
+        return {
+            "type": "transcript",
+            "text": transcript_text,
+            "is_final": response.is_final,
+            "confidence": alternative.confidence,
+            "timestamp": time.time()
+        }
 
 # Initialize services
 manager = ConnectionManager()
-deepgram_service = DeepgramService()
-audio_processor = AudioProcessor(deepgram_service)
+audio_processor = AudioProcessor()
 
 # FastAPI app setup
 app = FastAPI(
-    title="Spokenly Backend",
-    description="Real-time speech-to-text service with WebSocket support",
-    version="1.0.0"
+    title="Spokenly Backend (SDK Version)",
+    description="Real-time speech-to-text service using Deepgram Python SDK",
+    version="3.1.2"
 )
 
-# CORS middleware
 app.add_middleware(
     CORSMiddleware,
-    allow_origins=["*"],  # Configure appropriately for production
+    allow_origins=["*"],
     allow_credentials=True,
     allow_methods=["*"],
     allow_headers=["*"],
@@ -716,8 +335,7 @@ app.add_middleware(
 
 @app.get("/")
 async def root():
-    """Health check endpoint."""
-    return {"message": "Spokenly Backend is running", "version": "1.0.0"}
+    return {"message": "Spokenly Backend (SDK Version) is running"}
 
 @app.get("/health")
 async def health_check():
@@ -732,58 +350,28 @@ async def health_check():
 @app.websocket("/ws/{session_id}")
 async def websocket_endpoint(websocket: WebSocket, session_id: str, role: str = "recorder"):
     """Main WebSocket endpoint for audio streaming and transcription."""
+    if not await manager.connect(websocket, session_id, role):
+        return
+
     try:
-        # Validate role parameter
-        if role not in ["recorder", "viewer"]:
-            await websocket.close(code=1008, reason="Invalid role")
-            return
-        
-        # Connect and track the WebSocket
-        if not await manager.connect(websocket, session_id, role):
-            return
-        
-        # Send connection confirmation
         await manager.send_personal_message({
             "type": "connection_established",
             "session_id": session_id,
             "role": role,
-            "timestamp": time.time()
         }, websocket)
-        
-        # For viewers, send historical transcripts first
+
         if role == "viewer":
             await manager.send_session_history(websocket, session_id)
+            while True:
+                await websocket.receive_text()
         
-        # Only process audio stream for recorders
-        if role == "recorder":
+        elif role == "recorder":
             await audio_processor.process_audio_stream(websocket, session_id)
-        else:
-            # For viewers, keep the connection alive with proper error handling
-            try:
-                # Send a keep-alive ping every 30 seconds
-                while True:
-                    await asyncio.sleep(30)
-                    try:
-                        await websocket.ping()
-                    except Exception:
-                        # Connection is closed or error occurred
-                        break
-            except WebSocketDisconnect:
-                logger.info(f"Viewer disconnected normally for session {session_id}")
-            except Exception as e:
-                logger.error(f"Error in viewer connection for session {session_id}: {e}")
-        
+
     except WebSocketDisconnect:
         logger.info(f"WebSocket disconnected for session {session_id} (role: {role})")
     except Exception as e:
-        logger.error(f"Error in WebSocket handler for session {session_id} (role: {role}): {e}")
-        try:
-            await manager.send_personal_message({
-                "type": "error",
-                "message": "An error occurred during processing"
-            }, websocket)
-        except:
-            pass
+        logger.error(f"Error in WebSocket handler for session {session_id}: {e}")
     finally:
         manager.disconnect(websocket)
 
@@ -828,7 +416,7 @@ async def global_exception_handler(request, exc):
         status_code=500,
         content={"detail": "Internal server error"}
     )
-
+    
 if __name__ == "__main__":
     import uvicorn
     uvicorn.run(app, host="0.0.0.0", port=8000)
diff --git a/spokenly-backend/requirements.txt b/spokenly-backend/requirements.txt
index 355a8ac..9ccc64b 100644
--- a/spokenly-backend/requirements.txt
+++ b/spokenly-backend/requirements.txt
@@ -9,3 +9,4 @@ scipy==1.11.4
 librosa==0.10.1
 soundfile==0.12.1
 webrtcvad==2.0.10
+deepgram-sdk==0.3.0
diff --git a/spokenly-frontend/src/App.js b/spokenly-frontend/src/App.js
index ac11169..deb20b2 100644
--- a/spokenly-frontend/src/App.js
+++ b/spokenly-frontend/src/App.js
@@ -1,10 +1,10 @@
 import React, { useState, useRef, useEffect, useCallback } from 'react';
-import './App.css';
+import "./App.css"
 
 // Global variable to ensure sessionId is only generated once
 let globalSessionId = null;
 
-// Speaker color mapping
+// Speaker color mapping (currently unused but kept for potential future use)
 const speakerColors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD', '#98D8C8', '#F7DC6F'];
 
 function getSpeakerColor(speakerId) {
@@ -13,12 +13,10 @@ function getSpeakerColor(speakerId) {
 }
 
 function getInitialSessionId() {
-  // If already generated, return the existing one
   if (globalSessionId) {
     return globalSessionId;
   }
-  
-  // If share link present, use its sessionId; otherwise, generate a new one
+
   const urlParams = new URLSearchParams(window.location.search);
   const shareData = urlParams.get('share');
   if (shareData) {
@@ -32,185 +30,125 @@ function getInitialSessionId() {
       console.error('Error parsing share data:', error);
     }
   }
-  
-  // Generate a shorter, more shareable session ID
+
   const newSessionId = 'session-' + Math.random().toString(36).substr(2, 9);
   globalSessionId = newSessionId;
   return newSessionId;
 }
 
 const App = () => {
+  // --- Component State ---
   const [isRecording, setIsRecording] = useState(false);
   const [isConnected, setIsConnected] = useState(false);
-  // Transcript state: array of { text, isFinal }
-  const [transcriptChunks, setTranscriptChunks] = useState([]);
+  const [completedText, setCompletedText] = useState(''); // Holds the finalized paragraph
+
+
+  // State to hold the history of all completed sentences/paragraphs.
+  const [transcriptHistory, setTranscriptHistory] = useState([]);
+
+  // State to hold the current sentence being actively transcribed.
+  const [currentUtterance, setCurrentUtterance] = useState({ confirmed: '', interim: '' });
+  const utteranceRef = useRef({ confirmed: '', interim: '' }); // Ref to avoid stale state in callbacks.
+
   const [error, setError] = useState(null);
   const [connectionStatus, setConnectionStatus] = useState('disconnected');
   const [audioLevel, setAudioLevel] = useState(0);
   const [recordingTime, setRecordingTime] = useState(0);
-  const [recordingStartTime, setRecordingStartTime] = useState(null);
-  
-  // Customizable options for sharing
+
+  // Settings State
   const [chunkSize, setChunkSize] = useState(40);
   const [playbackSpeed, setPlaybackSpeed] = useState(1);
   const [autoScroll, setAutoScroll] = useState(true);
+
+  // Share Modal State
   const [showShareModal, setShowShareModal] = useState(false);
   const [shareUrl, setShareUrl] = useState('');
-  // Replace sessionId state with a ref
+
+  // --- Refs ---
   const sessionIdRef = useRef(getInitialSessionId());
   const sessionId = sessionIdRef.current;
-  
-  // Determine if this is a viewer BEFORE any connections
-  const isViewerRef = useRef(false);
+
   const [isViewer, setIsViewer] = useState(() => {
-    // Check if this is a share link on initial load
     const urlParams = new URLSearchParams(window.location.search);
-    const shareData = urlParams.get('share');
-    if (shareData) {
-      try {
-        const decodedData = JSON.parse(atob(shareData));
-        if (decodedData.sessionId) {
-          // This is a viewer joining an existing session
-          isViewerRef.current = true;
-          return true;
-        }
-      } catch (error) {
-        console.error('Error parsing share data:', error);
-      }
-    }
-    return false;
+    return urlParams.has('share');
   });
-  
-  // Transcript monitoring
-  const [lastTranscriptTime, setLastTranscriptTime] = useState(0);
-  const transcriptTimeoutRef = useRef(null);
-  
+
   const mediaRecorderRef = useRef(null);
   const websocketRef = useRef(null);
   const audioContextRef = useRef(null);
   const analyserRef = useRef(null);
   const animationFrameRef = useRef(null);
   const reconnectTimeoutRef = useRef(null);
-  const reconnectAttemptsRef = useRef(0);
   const recordingTimerRef = useRef(null);
+
+  // --- Constants ---
+  const reconnectAttemptsRef = useRef(0);
   const maxReconnectAttempts = 5;
   const reconnectDelay = 1000;
 
-  // Handle incoming WebSocket messages (chunked transcript logic)
+  // --- WebSocket Message Handling ---
   const handleWebSocketMessage = useCallback((data) => {
     switch (data.type) {
-      case 'connection_established':
-        break;
-      case 'session_history':
-        if (data.transcripts && data.transcripts.length > 0) {
-          // Only add finalized transcripts from history
-          setTranscriptChunks(
-            data.transcripts
-              .filter(t => t.is_final)
-              .map(t => ({
-                id: Date.now() + Math.random(),
-                text: t.text,
-                isFinal: t.is_final, // Use backend's is_final
-                timestamp: new Date(t.timestamp * 1000).toLocaleTimeString(),
-                speaker: t.speaker || null,
-                confidence: t.confidence || 0,
-                detectedLanguage: t.detected_language || null,
-                topics: t.topics || [],
-                speakerColor: getSpeakerColor(t.speaker)
-              }))
-          );
+      case 'transcript': {
+        // This handles the live updates for the *current* sentence.
+        if (data.is_final) {
+          setCompletedText(prev => (prev ? prev + ' ' : '') + data.text)
+          // The transcription service has finalized this part of the sentence.
+          utteranceRef.current.confirmed += ` ${data.text}`;
+          utteranceRef.current.interim = ''; // The interim prediction is now confirmed.
+
+        } else {
+          // This is a temporary, predictive part.
+          const newPart = data.text.substring(utteranceRef.current.confirmed.length);
+          utteranceRef.current.interim = newPart;
         }
+        // Update the UI with the latest state of the current sentence.
+        setCurrentUtterance({ ...utteranceRef.current });
         break;
-      case 'transcript': {
-        console.log('[WebSocket] Received transcript:', data);
-        
-        // Update last transcript time
-        setLastTranscriptTime(Date.now());
-        
-        // Clear any existing timeout
-        if (transcriptTimeoutRef.current) {
-          clearTimeout(transcriptTimeoutRef.current);
+      }
+
+      case 'utterance_end': {
+        // The speaker has paused, so the current sentence is considered complete.
+        const finishedUtterance = utteranceRef.current.confirmed;
+
+        if (finishedUtterance.trim()) {
+          // Add the completed sentence to our history array.
+          setTranscriptHistory(prevHistory => [...prevHistory, finishedUtterance]);
         }
-        
-        // Set a timeout to detect if transcripts stop
-        transcriptTimeoutRef.current = setTimeout(() => {
-          console.warn('⚠️ No transcripts received for 5 seconds - possible issue');
-          if (isRecording) {
-            console.log('🔍 Checking audio and connection status...');
-          }
-        }, 5000);
-        
-        // Always add the transcript, regardless of final status
-        setTranscriptChunks(prev => {
-          const newChunk = {
-            id: Date.now() + Math.random(),
-            text: data.text,
-            isFinal: data.is_final,
-            timestamp: new Date(data.timestamp * 1000).toLocaleTimeString(),
-            speaker: data.speaker || null,
-            confidence: data.confidence || 0,
-            detectedLanguage: data.detected_language || null,
-            topics: data.topics || [],
-            speakerColor: getSpeakerColor(data.speaker)
-          };
-
-          // Simplified logic: always add new transcripts
-          // Only replace the last chunk if it's an interim update
-          const last = prev[prev.length - 1];
-          if (last && !last.isFinal && !data.is_final) {
-            // Update the last interim chunk
-            return [...prev.slice(0, -1), newChunk];
-          } else {
-            // Add as new chunk
-            return [...prev, newChunk];
-          }
-        });
+
+        // Reset the utterance ref and state to prepare for the next sentence.
+        const resetUtterance = { confirmed: '', interim: '' };
+        utteranceRef.current = resetUtterance;
+        setCurrentUtterance(resetUtterance);
         break;
       }
+
       case 'error':
         setError(data.message);
         break;
+
       default:
         break;
     }
-  }, []);
+  }, []); // Empty dependency array ensures this function is created only once.
 
-  // WebSocket connection management
+  // --- WebSocket Connection Management ---
   const connectWebSocket = useCallback((userId = null) => {
-    // Prevent multiple simultaneous connection attempts
-    if (websocketRef.current?.readyState === WebSocket.OPEN || 
-        websocketRef.current?.readyState === WebSocket.CONNECTING) {
+    if (websocketRef.current?.readyState === WebSocket.OPEN || websocketRef.current?.readyState === WebSocket.CONNECTING) {
       return;
     }
+    if (reconnectTimeoutRef.current) clearTimeout(reconnectTimeoutRef.current);
 
-    // Clear any existing reconnection timeout
-    if (reconnectTimeoutRef.current) {
-      clearTimeout(reconnectTimeoutRef.current);
-      reconnectTimeoutRef.current = null;
-    }
-
-    // Close any existing connection first
-    if (websocketRef.current) {
-      websocketRef.current.close(1000, 'Reconnecting');
-      websocketRef.current = null;
-    }
-
-    // Use provided userId or current sessionId
     const currentSessionId = userId || sessionId;
-    const wsUrl = `ws://localhost:8000/ws/${currentSessionId}?role=${isViewer ? 'viewer' : 'recorder'}`;
-    console.log('Connecting to WebSocket with session ID:', currentSessionId, 'URL:', wsUrl, 'isViewer:', isViewer);
-    
+    const wsUrl = `ws://0.0.0.0:8000/ws/${currentSessionId}?role=${isViewer ? 'viewer' : 'recorder'}`;
+
     try {
       const ws = new WebSocket(wsUrl);
-
       ws.onopen = () => {
-        console.log('WebSocket connected successfully');
         setIsConnected(true);
         setConnectionStatus('connected');
-        setError(null);
         reconnectAttemptsRef.current = 0;
       };
-
       ws.onmessage = (event) => {
         try {
           const data = JSON.parse(event.data);
@@ -219,228 +157,116 @@ const App = () => {
           console.error('Failed to parse WebSocket message:', err);
         }
       };
-
       ws.onclose = (event) => {
-        console.log('WebSocket disconnected:', event.code, event.reason);
         setIsConnected(false);
-        setConnectionStatus('disconnected');
-        
-        // Don't reconnect if it's a "Too many connections" error or manual close
         if (event.code === 1008) {
-          console.log('Too many connections error - not reconnecting');
           setConnectionStatus('connection limit reached');
           return;
         }
-        
-        // Only attempt reconnection if not manually closed and we're not already reconnecting
-        if (event.code !== 1000 && 
-            reconnectAttemptsRef.current < maxReconnectAttempts && 
-            !reconnectTimeoutRef.current) {
+        if (event.code !== 1000 && reconnectAttemptsRef.current < maxReconnectAttempts) {
           reconnectAttemptsRef.current++;
           setConnectionStatus(`reconnecting (${reconnectAttemptsRef.current}/${maxReconnectAttempts})`);
-          
-          // Use exponential backoff to prevent rapid reconnections
           const delay = reconnectDelay * Math.pow(2, reconnectAttemptsRef.current - 1);
-          reconnectTimeoutRef.current = setTimeout(() => {
-            reconnectTimeoutRef.current = null;
-            connectWebSocket(userId);
-          }, delay);
+          reconnectTimeoutRef.current = setTimeout(() => connectWebSocket(userId), delay);
+        } else {
+          setConnectionStatus('disconnected');
         }
       };
-
-      ws.onerror = (error) => {
-        console.error('WebSocket error:', error);
-        setError('WebSocket connection failed');
-        setConnectionStatus('error');
+      ws.onerror = (e) => {
+        if( websocketRef.current?.readyState !== WebSocket.CONNECTING ){
+          console.error(websocketRef.current?.readyState)
+          setError('WebSocket connection failed');
+          setConnectionStatus('error');
+        }
       };
-
       websocketRef.current = ws;
     } catch (error) {
-      console.error('Failed to create WebSocket:', error);
       setError('Failed to create WebSocket connection');
       setConnectionStatus('error');
     }
   }, [sessionId, isViewer, handleWebSocketMessage]);
 
   const disconnectWebSocket = useCallback(() => {
-    if (reconnectTimeoutRef.current) {
-      clearTimeout(reconnectTimeoutRef.current);
-    }
-    
-    if (websocketRef.current) {
-      websocketRef.current.close(1000, 'User initiated disconnect');
-      websocketRef.current = null;
-    }
-    
-    setIsConnected(false);
-    setConnectionStatus('disconnected');
+    if (reconnectTimeoutRef.current) clearTimeout(reconnectTimeoutRef.current);
+    websocketRef.current?.close(1000, 'User initiated disconnect');
   }, []);
 
-  // Audio recording and streaming
+  // --- Audio Recording & Streaming ---
   const startRecording = useCallback(async () => {
     try {
       setError(null);
-      setRecordingStartTime(Date.now());
       setRecordingTime(0);
-      
-      // Request microphone access with optimal settings for accuracy
+
       const stream = await navigator.mediaDevices.getUserMedia({
-        audio: {
-          echoCancellation: true,
-          noiseSuppression: true,
-          autoGainControl: true,
-          sampleRate: 16000, // 16kHz for Deepgram compatibility
-          channelCount: 1
-        }
+        audio: { echoCancellation: true, noiseSuppression: true, autoGainControl: true, sampleRate: 16000, channelCount: 1 }
       });
 
-      // Set up audio analysis for visualization
+      // Audio level visualization setup
       audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)();
       const source = audioContextRef.current.createMediaStreamSource(stream);
       analyserRef.current = audioContextRef.current.createAnalyser();
       analyserRef.current.fftSize = 256;
       source.connect(analyserRef.current);
 
-      // Start audio level monitoring
       const updateAudioLevel = () => {
-        if (analyserRef.current && isRecording) {
+        if (analyserRef.current && mediaRecorderRef.current) { // Check if still recording
           const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
           analyserRef.current.getByteFrequencyData(dataArray);
-          
-          const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
+          const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;
           setAudioLevel(average);
-          
           animationFrameRef.current = requestAnimationFrame(updateAudioLevel);
         }
       };
 
-      // Use Web Audio API to get raw PCM data
-      const audioContext = new (window.AudioContext || window.webkitAudioContext)({
-        sampleRate: 16000 // Force 16kHz sample rate
-      });
+      // Audio processing for WebSocket
+      const audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
       const audioSource = audioContext.createMediaStreamSource(stream);
       const processor = audioContext.createScriptProcessor(4096, 1, 1);
-      
+
       processor.onaudioprocess = (event) => {
         if (websocketRef.current?.readyState === WebSocket.OPEN) {
-          // Get raw PCM data from the audio buffer
-          const inputBuffer = event.inputBuffer;
-          const inputData = inputBuffer.getChannelData(0);
-          
-          // Convert float32 to int16 (linear16 format) with amplification
+          const inputData = event.inputBuffer.getChannelData(0);
           const int16Array = new Int16Array(inputData.length);
-          const amplification = 2.0; // Increase volume
           for (let i = 0; i < inputData.length; i++) {
-            int16Array[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768 * amplification));
+            int16Array[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768));
           }
-          
-          // Debug: log audio data occasionally
-          if (Math.random() < 0.05) { // Log 5% of the time
-            const maxValue = Math.max(...inputData);
-            const minValue = Math.min(...inputData);
-            const maxInt16 = Math.max(...int16Array);
-            const minInt16 = Math.min(...int16Array);
-            
-            console.log('Audio data:', {
-              samples: inputData.length,
-              sampleRate: inputBuffer.sampleRate,
-              maxValue: maxValue,
-              minValue: minValue,
-              maxInt16: maxInt16,
-              minInt16: minInt16,
-              bufferSize: int16Array.buffer.byteLength,
-              hasAudio: maxValue > 0.01 || minValue < -0.01
-            });
-            
-            if (maxValue < 0.01 && minValue > -0.01) {
-              console.warn('⚠️ Audio appears to be silent');
-            } else if (maxValue > 0.1 || minValue < -0.1) {
-              console.log('✅ Good audio detected');
-            }
-          }
-          
-          // Send the raw audio data
           websocketRef.current.send(int16Array.buffer);
         }
       };
-      
+
       audioSource.connect(processor);
       processor.connect(audioContext.destination);
-      
-      // Store references for cleanup
-      mediaRecorderRef.current = {
-        audioContext,
-        audioSource,
-        processor,
-        stream
-      };
 
-      // Web Audio API is already processing audio automatically
+      mediaRecorderRef.current = { audioContext, audioSource, processor, stream };
       setIsRecording(true);
       updateAudioLevel();
 
-      // Start recording timer
-      recordingTimerRef.current = setInterval(() => {
-        setRecordingTime(prev => prev + 1);
-      }, 1000);
+      recordingTimerRef.current = setInterval(() => setRecordingTime(prev => prev + 1), 1000);
 
-      // Ensure WebSocket is connected
-      if (!isConnected) {
-        connectWebSocket();
-      }
+      if (!isConnected) connectWebSocket();
 
     } catch (err) {
-      console.error('Failed to start recording:', err);
       setError(err.message || 'Failed to access microphone');
     }
   }, [isConnected, connectWebSocket]);
 
   const stopRecording = useCallback(() => {
-    if (mediaRecorderRef.current) {
-      // Clean up Web Audio API components
-      if (mediaRecorderRef.current.processor) {
-        mediaRecorderRef.current.processor.disconnect();
-      }
-      if (mediaRecorderRef.current.audioSource) {
-        mediaRecorderRef.current.audioSource.disconnect();
-      }
-      if (mediaRecorderRef.current.audioContext) {
-        mediaRecorderRef.current.audioContext.close();
-      }
-      if (mediaRecorderRef.current.stream) {
-        mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop());
-      }
-    }
-
-    if (audioContextRef.current) {
-      audioContextRef.current.close();
-      audioContextRef.current = null;
-    }
+    if (animationFrameRef.current) cancelAnimationFrame(animationFrameRef.current);
+    if (recordingTimerRef.current) clearInterval(recordingTimerRef.current);
 
-    if (animationFrameRef.current) {
-      cancelAnimationFrame(animationFrameRef.current);
-      animationFrameRef.current = null;
-    }
-
-    if (recordingTimerRef.current) {
-      clearInterval(recordingTimerRef.current);
-      recordingTimerRef.current = null;
+    if (mediaRecorderRef.current) {
+      mediaRecorderRef.current.stream?.getTracks().forEach(track => track.stop());
+      mediaRecorderRef.current.processor?.disconnect();
+      mediaRecorderRef.current.audioSource?.disconnect();
+      mediaRecorderRef.current.audioContext?.close();
+      mediaRecorderRef.current = null;
     }
 
-    // Clear transcript monitoring
-    if (transcriptTimeoutRef.current) {
-      clearTimeout(transcriptTimeoutRef.current);
-      transcriptTimeoutRef.current = null;
-    }
+    audioContextRef.current?.close();
 
     setIsRecording(false);
     setAudioLevel(0);
     setRecordingTime(0);
-    setRecordingStartTime(null);
-  }, []);
-
-  const clearTranscripts = useCallback(() => {
-    setTranscriptChunks([]);
   }, []);
 
   const copyTranscript = useCallback((text) => {
@@ -450,34 +276,21 @@ const App = () => {
     });
   }, []);
 
+  // --- Utility & Lifecycle ---
   const generateShareLink = useCallback(() => {
+    const fullTranscript = [...transcriptHistory, currentUtterance.confirmed].join(' ');
     const shareData = {
       sessionId: sessionId,
-      transcripts: transcriptChunks.map(t => ({
-        text: t.text,
-        timestamp: t.timestamp,
-        speakerColor: t.speakerColor
-      })),
-      settings: {
-        chunkSize,
-        playbackSpeed,
-        autoScroll
-      },
-      isLiveRecording: isRecording,
-      recordingTime: recordingTime,
-      timestamp: Date.now()
+      transcripts: [{ text: fullTranscript, timestamp: new Date().toLocaleTimeString() }],
+      settings: { chunkSize, playbackSpeed, autoScroll },
     };
-    
     const encodedData = btoa(JSON.stringify(shareData));
-    const shareUrl = `${window.location.origin}${window.location.pathname}?share=${encodedData}`;
-    console.log('Generated share link with sessionId:', sessionId, 'URL:', shareUrl);
-    return shareUrl;
-  }, [sessionId, transcriptChunks, chunkSize, playbackSpeed, autoScroll, isRecording, recordingTime]);
+    return `${window.location.origin}${window.location.pathname}?share=${encodedData}`;
+  }, [sessionId, transcriptHistory, currentUtterance, chunkSize, playbackSpeed, autoScroll]);
 
   const copyShareLink = useCallback(() => {
     const url = generateShareLink();
     navigator.clipboard.writeText(url).then(() => {
-      console.log('Share link copied to clipboard');
       setShareUrl(url);
       setShowShareModal(true);
     });
@@ -486,189 +299,104 @@ const App = () => {
   const loadSharedData = useCallback(() => {
     const urlParams = new URLSearchParams(window.location.search);
     const shareData = urlParams.get('share');
-    
     if (shareData) {
       try {
         const decodedData = JSON.parse(atob(shareData));
-        
-        if (decodedData.transcripts) {
-          setTranscriptChunks(decodedData.transcripts.map(t => ({
-            id: Date.now() + Math.random(),
-            text: t.text,
-            isFinal: true,
-            timestamp: t.timestamp || new Date().toLocaleTimeString(),
-            speakerColor: t.speakerColor || 'blue'
-          })));
+        if (decodedData.transcripts && decodedData.transcripts.length > 0) {
+          // Simple load: split by sentences. A more robust solution might be needed.
+          setTranscriptHistory(decodedData.transcripts[0].text.split('. ').filter(Boolean));
         }
         if (decodedData.settings) {
           setChunkSize(decodedData.settings.chunkSize || 40);
           setPlaybackSpeed(decodedData.settings.playbackSpeed || 1);
           setAutoScroll(decodedData.settings.autoScroll !== false);
         }
-        
-        // Show info if this was a live recording
-        if (decodedData.isLiveRecording) {
-          console.log('Loaded shared live recording session:', decodedData.sessionId);
-          console.log('Current sessionId:', sessionId);
-          console.log('isViewer already set to:', isViewer);
-        }
       } catch (error) {
         console.error('Failed to load shared data:', error);
       }
     }
-  }, [sessionId]);
+  }, []);
 
-  // Format recording time
   const formatTime = (seconds) => {
     const mins = Math.floor(seconds / 60);
     const secs = seconds % 60;
     return `${mins}:${secs.toString().padStart(2, '0')}`;
   };
 
-  // Cleanup on unmount
+  const handleChunkChange = (amount) => {
+    setChunkSize(prev => Math.max(10, Math.min(100, prev + amount)));
+  }
+
   useEffect(() => {
+    // Initial connection attempt
+    connectWebSocket();
+    // Load any shared data from URL
+    loadSharedData();
+
+    // Cleanup on unmount
     return () => {
       stopRecording();
       disconnectWebSocket();
     };
-  }, [stopRecording, disconnectWebSocket]);
-
-  // Connect WebSocket on mount (for both recorder and viewer)
-  useEffect(() => {
-    console.log('WebSocket connection effect triggered, sessionId:', sessionId, 'isViewer:', isViewer);
-    if (!websocketRef.current && !reconnectTimeoutRef.current) {
-      connectWebSocket();
-    }
-    // eslint-disable-next-line
-  }, [isViewer]);
-
-  // Load shared data on mount
-  useEffect(() => {
-    loadSharedData();
-  }, [loadSharedData]);
-
-  // Connection health monitoring
-  useEffect(() => {
-    const healthCheck = setInterval(() => {
-      if (websocketRef.current && websocketRef.current.readyState === WebSocket.OPEN) {
-        // Connection is healthy
-        setConnectionStatus('connected');
-      } else if (websocketRef.current && websocketRef.current.readyState === WebSocket.CLOSED) {
-        // Connection is closed, attempt to reconnect
-        console.log('Connection health check: WebSocket is closed, attempting reconnect');
-        if (!reconnectTimeoutRef.current) {
-          connectWebSocket();
-        }
-      }
-    }, 10000); // Check every 10 seconds
+  }, [connectWebSocket, disconnectWebSocket, stopRecording, loadSharedData]);
 
-    return () => clearInterval(healthCheck);
-  }, [connectWebSocket]);
+  const hasContent = transcriptHistory.length > 0 || currentUtterance.confirmed || currentUtterance.interim;
 
   return (
     <div className="App">
-      {/* Top Control Panel */}
-      <div className="control-panel">
+      {/* <style>{AppStyles}</style> */}
+      <header className="control-panel">
         <div className="control-left">
           {isViewer ? (
-            <div className="viewer-status">
-              <span className="live-indicator">🔴</span>
-              <span>Live Viewing Session</span>
-            </div>
+            <div className="viewer-status"><span className="live-indicator">🔴</span><span>Live Viewing</span></div>
           ) : (
-            <>
-              {isRecording ? (
-                <button className="stop-button" onClick={stopRecording}>
-                  <span className="stop-icon">⏹</span>
-                  Stop Recording
-                </button>
-              ) : (
-                <button 
-                  className="start-button" 
-                  onClick={startRecording}
-                  disabled={!isConnected && connectionStatus !== 'reconnecting'}
-                >
-                  <span className="mic-icon">🎤</span>
-                  Start Recording
-                </button>
-              )}
-            </>
-          )}
-          
-          {isRecording && (
-            <div className="recording-progress">
-              <label>Recording Progress</label>
-              <div className="progress-bar">
-                <div 
-                  className="progress-fill"
-                  style={{ width: `${(recordingTime / 60) * 100}%` }}
-                ></div>
-              </div>
-            </div>
+            isRecording ? (
+              <button className="stop-button" onClick={stopRecording}>⏹ Stop Recording</button>
+            ) : (
+              <button className="start-button" onClick={startRecording} disabled={connectionStatus === 'disconnected'}>🎤 Start Recording</button>
+            )
           )}
         </div>
-
         <div className="control-right">
-          <div className={`connection-status ${connectionStatus}`}>
-            <span className="status-dot"></span>
-            {connectionStatus === 'connected' ? 'Connected' : connectionStatus}
-          </div>
-          
-          {isRecording && (
-            <div className="recording-timer">
-              {formatTime(recordingTime)} / 10:00
-            </div>
-          )}
+          <div className={`connection-status ${connectionStatus}`}><span className="status-dot"></span>{connectionStatus}</div>
+          {isRecording && <div className="recording-timer">{formatTime(recordingTime)}</div>}
         </div>
-      </div>
-
-      {/* Customizable Controls */}
-      <div className="controls-section">
-        <div className="control-group">
-          <label>Chunk Size:</label>
-          <div className="chunk-controls">
-            <button 
-              className="control-btn"
-              onClick={() => setChunkSize(prev => Math.max(10, prev - 5))}
-              disabled={chunkSize <= 10}
-            >
-              -
-            </button>
-            <span className="control-value">{chunkSize}</span>
-            <button 
-              className="control-btn"
-              onClick={() => setChunkSize(prev => Math.min(100, prev + 5))}
-              disabled={chunkSize >= 100}
+      </header>
+
+      {!isViewer && (
+        <section className="controls-section">
+          <div className="control-group">
+            <label>Chunk Size (ms)</label>
+            <div className="chunk-controls">
+              <button className="control-btn" onClick={() => handleChunkChange(-10)} disabled={chunkSize <= 10}>-</button>
+              <span className="control-value">{chunkSize}</span>
+              <button className="control-btn" onClick={() => handleChunkChange(10)} disabled={chunkSize >= 100}>+</button>
+            </div>
+          </div>
+          <div className="control-group">
+            <label>Speed</label>
+            <input
+              type="range"
+              min="0.5"
+              max="2"
+              step="0.1"
+              value={playbackSpeed}
+              className="speed-slider"
+              onChange={(e) => setPlaybackSpeed(parseFloat(e.target.value))}
+            />
+            <span className="control-value">{playbackSpeed.toFixed(1)}x</span>
+          </div>
+          <div className="control-group">
+            <label>Auto-Scroll</label>
+            <button
+              className={`toggle-btn ${autoScroll ? 'active' : ''}`}
+              onClick={() => setAutoScroll(!autoScroll)}
             >
-              +
+              {autoScroll ? 'On' : 'Off'}
             </button>
           </div>
-        </div>
-        
-        <div className="control-group">
-          <label>Speed:</label>
-          <input
-            type="range"
-            min="0.5"
-            max="2"
-            step="0.1"
-            value={playbackSpeed}
-            onChange={(e) => setPlaybackSpeed(parseFloat(e.target.value))}
-            className="speed-slider"
-          />
-          <span className="control-value">{playbackSpeed}x</span>
-        </div>
-        
-        <div className="control-group">
-          <label>Auto-scroll:</label>
-          <button 
-            className={`toggle-btn ${autoScroll ? 'active' : ''}`}
-            onClick={() => setAutoScroll(!autoScroll)}
-          >
-            {autoScroll ? 'ON' : 'OFF'}
-          </button>
-        </div>
-      </div>
+        </section>
+      )}
 
       {/* Share Button - Only show for recorders, not viewers */}
       {!isViewer && (
@@ -683,119 +411,86 @@ const App = () => {
         </div>
       )}
 
-      {/* Share Modal */}
       {showShareModal && (
         <div className="modal-overlay" onClick={() => setShowShareModal(false)}>
           <div className="modal-content" onClick={(e) => e.stopPropagation()}>
-            <h3>{isRecording ? 'Live Share Link Generated!' : 'Share Link Generated!'}</h3>
-            <p>
-              {isRecording 
-                ? 'Your live recording session is now shareable with real-time updates.' 
-                : 'Your transcript has been shared with the following settings:'
-              }
-            </p>
-            <div className="share-settings">
-              <div>Chunk Size: {chunkSize}</div>
-              <div>Speed: {playbackSpeed}x</div>
-              <div>Auto-scroll: {autoScroll ? 'ON' : 'OFF'}</div>
-              {isRecording && (
-                <>
-                  <div>Status: 🔴 Live Recording</div>
-                  <div>Recording Time: {formatTime(recordingTime)}</div>
-                </>
-              )}
-              {transcriptChunks.length > 0 && (
-                <div>Transcripts: {transcriptChunks.length} entries</div>
-              )}
-            </div>
+            <h3>Share Link Generated!</h3>
             <div className="share-url">
-              <input 
-                type="text" 
-                value={shareUrl} 
-                readOnly 
-                className="url-input"
-              />
-              <button 
-                className="copy-url-btn"
-                onClick={() => navigator.clipboard.writeText(shareUrl)}
-              >
-                Copy
-              </button>
+              <input type="text" value={shareUrl} readOnly />
+              <button onClick={() => navigator.clipboard.writeText(shareUrl)}>Copy</button>
             </div>
-            <button 
-              className="close-modal-btn"
-              onClick={() => setShowShareModal(false)}
-            >
-              Close
-            </button>
+            <button className="close-modal-btn" onClick={() => setShowShareModal(false)}>Close</button>
           </div>
         </div>
       )}
 
-      {/* Error Display */}
-      {error && (
-        <div className="error-message">
-          ⚠️ {error}
-        </div>
-      )}
+      {error && <div className="error-message">⚠️ {error}</div>}
 
-      {/* Transcripts Container */}
-      <div className="transcripts-container">
-        {transcriptChunks.length === 0 ? (
-          <div className="empty-state">
-            <p>Start recording to see live transcriptions...</p>
-          </div>
-        ) : (
+      <main className="transcripts-container">
+        {hasContent ? (
           <div className="transcripts-list">
-            {transcriptChunks.map((chunk, index) => (
+            {/* Render the history of completed transcripts */}
+            {transcriptHistory.map((text, index) => (
               <div
-                key={chunk.id}
-                className={`transcript-card ${chunk.isFinal ? 'final' : 'interim'}`}
+                key={index}
+                className="transcript-card final"
               >
                 <div className="transcript-number">{index + 1}</div>
                 <div className="transcript-content">
                   <div className="transcript-header">
-                    <span 
-                      className="speaker-dot" 
-                      style={{ backgroundColor: chunk.speakerColor }}
-                      title={chunk.speaker ? `Speaker ${chunk.speaker}` : 'Unknown Speaker'}
-                    ></span>
-                    <span className="transcript-text">{chunk.text}</span>
-                    <button 
+                    {/* NOTE: The speaker dot is omitted here.
+              To re-add it, your `transcriptHistory` would need to contain speaker objects.
+              e.g., <span className="speaker-dot" style={{ backgroundColor: item.speakerColor }}></span> 
+            */}
+                    <span className="transcript-text">{text}</span>
+                    <button
                       className="copy-transcript-btn"
-                      onClick={() => copyTranscript(chunk.text)}
+                      onClick={() => copyTranscript(text)}
                       title="Copy transcript"
                     >
                       📋
                     </button>
                   </div>
-                  <div className="transcript-meta">
-                    <div className="transcript-timestamp">{chunk.timestamp}</div>
-                    {chunk.speaker && (
-                      <div className="speaker-info">Speaker {chunk.speaker}</div>
-                    )}
-                    {chunk.confidence > 0 && (
-                      <div className="confidence-score">
-                        Confidence: {Math.round(chunk.confidence * 100)}%
-                      </div>
-                    )}
-                    {chunk.detectedLanguage && (
-                      <div className="language-info">
-                        Language: {chunk.detectedLanguage}
-                      </div>
-                    )}
-                    {chunk.topics && chunk.topics.length > 0 && (
-                      <div className="topics-info">
-                        Topics: {chunk.topics.join(', ')}
-                      </div>
-                    )}
-                  </div>
+                  {/* NOTE: The metadata section (timestamps, confidence, etc.) is omitted.
+            This data is not available in the new `transcriptHistory` array of strings.
+            You would need to enrich the data structure to display it.
+          */}
                 </div>
               </div>
             ))}
+
+            {/* Render the current, live transcript */}
+            {(currentUtterance.confirmed || currentUtterance.interim) && (
+              <div className="transcript-card interim">
+                <div className="transcript-number">{transcriptHistory.length + 1}</div>
+                <div className="transcript-content">
+                  <div className="transcript-header">
+                    <span className="transcript-text">
+                      {/* This preserves your new logic for displaying confirmed vs. interim text */}
+                      <span>{currentUtterance.confirmed ? (completedText ? " " : "") + currentUtterance.confirmed : ""}</span>
+                      <span className="interim-text">{currentUtterance.interim}</span>
+                      <span>&nbsp;</span>
+                    </span>
+                    <button
+                      className="copy-transcript-btn"
+                      onClick={() => copyTranscript(
+                        `${currentUtterance.confirmed || ''} ${currentUtterance.interim || ''}`.trim()
+                      )}
+                      title="Copy transcript"
+                    >
+                      📋
+                    </button>
+                  </div>
+                </div>
+              </div>
+            )}
+          </div>
+        ) : (
+          <div className="empty-state">
+            <p>Start recording to see live transcriptions...</p>
           </div>
         )}
-      </div>
+      </main>
     </div>
   );
 };
-- 
2.39.5 (Apple Git-154)


From 9debd565f103cda10d7799d6fdf4028dd6d02ffd Mon Sep 17 00:00:00 2001
From: Nikunj patriwala <nikunj@ox.security>
Date: Thu, 14 Aug 2025 01:47:09 +0530
Subject: [PATCH 3/4] Remove the unncessory Logs

---
 spokenly-backend/main.py | 19 -------------------
 1 file changed, 19 deletions(-)

diff --git a/spokenly-backend/main.py b/spokenly-backend/main.py
index 534b85c..e43e870 100644
--- a/spokenly-backend/main.py
+++ b/spokenly-backend/main.py
@@ -204,19 +204,7 @@ class AudioProcessor:
                 logger.info(f"Deepgram connection opened for session {session_id}.")
 
             async def on_message(cls,result: LiveResultResponse, **kwargs):
-                logger.info(f" on_message ".center( 80 , "=" ) )
-                
-                logger.info(f" Result Data ".center( 80 , "-" ) )
-                logger.info(result)
-                logger.info(f"-"*80)
-
                 transcript_data = self._extract_transcript(result)
-                
-                logger.info(f" Transcipt Data ".center( 80 , "~" ) )
-                logger.info(transcript_data)
-                logger.info(f"~"*80)
-
-                logger.info(f"=" * 80)
 
                 if transcript_data:
                     if transcript_data["is_final"]:
@@ -224,19 +212,12 @@ class AudioProcessor:
                     await manager.broadcast_to_session(transcript_data, session_id)
 
             async def on_metadata(cls, metadata: MetadataResponse, **kwargs):
-                logger.info(f" on_metadata ".center( 80 , "=" ) )
-                logger.info(metadata)
-                logger.info(f"=" * 80)
                 logger.info(f"Deepgram metadata received for session {session_id}: {metadata}")
 
             async def on_utterance_end(cls, utterance_end: UtteranceEndResponse, **kwargs):
-                logger.info(f" on_metadata ".center( 80 , "=" ) )
-                logger.info(utterance_end)
-                logger.info(f"=" * 80)
                 await manager.broadcast_to_session({"type": "utterance_end"}, session_id)
 
             async def on_error(cls, error: DeepgramError, **kwargs):
-                logger.error(f"Deepgram error for session {session_id}: {error}")
                 await manager.broadcast_to_session({"type": "error", "message": str(error)}, session_id)
 
             async def on_close(cls, close, **kwargs):
-- 
2.39.5 (Apple Git-154)


From f0bcc6c1ddb5b004d36e0d5eec134a407df6b4ad Mon Sep 17 00:00:00 2001
From: ravimaha513 <ravimaha513@gmail.com>
Date: Wed, 13 Aug 2025 23:58:45 -0400
Subject: [PATCH 4/4] feat: implement ultra-low latency transcription with
 clean UI

- Remove latency profile dropdown and hardcode to ultra-low latency
- Fix Deepgram connection issues by removing invalid parameters
- Implement interim results display with visual distinction
- Remove copy buttons and latency indicators for cleaner UI
- Add ultra-low latency styling for confirmed vs interim text
- Update share functionality to include interim results
- Add comprehensive latency optimization documentation
- Fix backend health check endpoint
- Optimize audio processing for lower latency
---
 spokenly-backend/LATENCY_OPTIMIZATION.md | 154 +++++++++++++++++++++++
 spokenly-backend/latency_config.py       |  71 +++++++++++
 spokenly-backend/main.py                 |  43 +++++--
 spokenly-frontend/src/App.css            |  71 ++++++++---
 spokenly-frontend/src/App.js             | 121 ++++++++++--------
 5 files changed, 381 insertions(+), 79 deletions(-)
 create mode 100644 spokenly-backend/LATENCY_OPTIMIZATION.md
 create mode 100644 spokenly-backend/latency_config.py

diff --git a/spokenly-backend/LATENCY_OPTIMIZATION.md b/spokenly-backend/LATENCY_OPTIMIZATION.md
new file mode 100644
index 0000000..86ca500
--- /dev/null
+++ b/spokenly-backend/LATENCY_OPTIMIZATION.md
@@ -0,0 +1,154 @@
+# Spokenly Latency Optimization Guide
+
+## Overview
+
+This guide explains the latency optimizations implemented in Spokenly to provide real-time transcription with minimal delay. The system now supports multiple latency profiles that you can choose from based on your specific needs.
+
+## Key Optimizations Made
+
+### 1. Backend Optimizations
+
+#### Deepgram Configuration
+- **Reduced utterance_end_ms**: From 5000ms to 1000ms (configurable)
+- **Aggressive endpointing**: From 300ms to 150ms (configurable)
+- **Enabled interim results**: More frequent partial transcriptions
+- **Disabled unnecessary features**: Speaker diarization, alternatives, etc.
+- **Optimized VAD**: Less aggressive voice activity detection
+
+#### Audio Processing
+- **Smaller audio buffers**: Reduced from 4096 to 1024 samples
+- **Faster chunk processing**: Reduced chunk size from 40ms to 20ms
+- **Optimized sample rate**: Maintained at 16kHz for best performance
+
+### 2. Frontend Optimizations
+
+#### Real-time Display
+- **Immediate interim display**: Shows partial results as they come in
+- **Reduced UI updates**: More efficient state management
+- **Smaller audio chunks**: Faster processing and transmission
+
+#### WebSocket Handling
+- **Optimized message processing**: Faster parsing and display
+- **Reduced buffer sizes**: Smaller audio chunks for lower latency
+
+## Latency Profiles
+
+The system now includes four pre-configured latency profiles:
+
+### 1. Ultra-Low Latency
+- **Description**: Fastest possible transcription with minimal accuracy trade-offs
+- **Use Case**: Live presentations, real-time conversations
+- **Settings**:
+  - Utterance end: 500ms
+  - Endpointing: 100ms
+  - Audio buffer: 512 samples
+  - Chunk size: 10ms
+
+### 2. Low Latency (Default)
+- **Description**: Good balance between speed and accuracy
+- **Use Case**: General transcription, meetings, interviews
+- **Settings**:
+  - Utterance end: 1000ms
+  - Endpointing: 150ms
+  - Audio buffer: 1024 samples
+  - Chunk size: 20ms
+
+### 3. Standard
+- **Description**: Balanced approach with good accuracy
+- **Use Case**: Documentation, content creation
+- **Settings**:
+  - Utterance end: 2000ms
+  - Endpointing: 300ms
+  - Audio buffer: 2048 samples
+  - Chunk size: 40ms
+
+### 4. High Accuracy
+- **Description**: Slower but more accurate transcription
+- **Use Case**: Legal proceedings, medical documentation
+- **Settings**:
+  - Utterance end: 5000ms
+  - Endpointing: 500ms
+  - Audio buffer: 4096 samples
+  - Chunk size: 50ms
+
+## How to Use
+
+### Via Frontend
+1. Start the application
+2. Look for the "Latency Profile" dropdown in the controls section
+3. Select your preferred profile
+4. The system will automatically adjust all settings
+
+### Via API
+```bash
+# Get available profiles
+curl http://localhost:8000/latency-profiles
+
+# Set a specific profile
+curl -X POST http://localhost:8000/set-latency-profile/ULTRA_LOW
+```
+
+## Environment Variables
+
+You can also configure latency settings directly in the `.env` file:
+
+```env
+# Low Latency Configuration
+ENABLE_INTERIM_RESULTS=true
+UTTERANCE_END_MS=1000
+ENDPOINTING=150
+INTERIM_RESULTS_INTERVAL=100
+VAD_EVENTS=true
+VAD_AGGRESSIVENESS=1
+```
+
+## Performance Tips
+
+### For Ultra-Low Latency:
+1. Use a wired internet connection
+2. Close other bandwidth-intensive applications
+3. Use a high-quality microphone
+4. Speak clearly and at a moderate pace
+
+### For Best Accuracy:
+1. Use the "High Accuracy" profile
+2. Ensure quiet background environment
+3. Speak at a normal pace with clear pronunciation
+4. Use a high-quality microphone
+
+## Troubleshooting
+
+### High Latency Issues:
+1. Check your internet connection speed
+2. Try switching to a lower latency profile
+3. Close other applications using the microphone
+4. Check if your browser supports WebRTC
+
+### Accuracy Issues:
+1. Switch to a higher accuracy profile
+2. Improve microphone quality
+3. Reduce background noise
+4. Speak more clearly and at a normal pace
+
+## Technical Details
+
+### Deepgram Parameters Explained:
+- **utterance_end_ms**: How long to wait after silence before finalizing a transcript
+- **endpointing**: How quickly to detect the end of speech
+- **interim_results**: Whether to send partial results before finalizing
+- **vad_events**: Voice activity detection events
+- **vad_aggressiveness**: How sensitive the voice detection is (1=least, 3=most)
+
+### Audio Processing:
+- **Buffer size**: Smaller buffers = lower latency but more CPU usage
+- **Chunk size**: Smaller chunks = faster transmission but more overhead
+- **Sample rate**: 16kHz provides good quality with reasonable processing
+
+## Future Improvements
+
+Planned enhancements for even lower latency:
+1. WebRTC optimization
+2. Audio compression improvements
+3. Parallel processing for multiple audio streams
+4. Machine learning-based latency prediction
+5. Adaptive latency based on network conditions
diff --git a/spokenly-backend/latency_config.py b/spokenly-backend/latency_config.py
new file mode 100644
index 0000000..69ce745
--- /dev/null
+++ b/spokenly-backend/latency_config.py
@@ -0,0 +1,71 @@
+"""
+Latency Configuration Profiles for Spokenly
+This file contains different configuration profiles optimized for various use cases.
+"""
+
+class LatencyProfiles:
+    """Different latency profiles for various use cases."""
+    
+    # Ultra-low latency profile - fastest possible transcription
+    ULTRA_LOW = {
+        "utterance_end_ms": "500",  # Very short pause detection
+        "endpointing": 100,  # Very aggressive endpointing
+        "interim_results_interval": 50,  # Very frequent interim results
+        "vad_aggressiveness": 1,  # Least aggressive VAD
+        "audio_buffer_size": 512,  # Smallest audio buffer
+        "chunk_size": 10,  # Smallest chunk size
+        "description": "Ultra-low latency - fastest possible transcription with minimal accuracy trade-offs"
+    }
+    
+    # Low latency profile - balanced speed and accuracy
+    LOW = {
+        "utterance_end_ms": "1000",  # Short pause detection
+        "endpointing": 150,  # Aggressive endpointing
+        "interim_results_interval": 100,  # Frequent interim results
+        "vad_aggressiveness": 1,  # Least aggressive VAD
+        "audio_buffer_size": 1024,  # Small audio buffer
+        "chunk_size": 20,  # Small chunk size
+        "description": "Low latency - good balance between speed and accuracy"
+    }
+    
+    # Standard profile - balanced approach
+    STANDARD = {
+        "utterance_end_ms": "2000",  # Standard pause detection
+        "endpointing": 300,  # Standard endpointing
+        "interim_results_interval": 200,  # Standard interim results
+        "vad_aggressiveness": 2,  # Standard VAD
+        "audio_buffer_size": 2048,  # Standard audio buffer
+        "chunk_size": 40,  # Standard chunk size
+        "description": "Standard - balanced approach with good accuracy"
+    }
+    
+    # High accuracy profile - slower but more accurate
+    HIGH_ACCURACY = {
+        "utterance_end_ms": "5000",  # Long pause detection
+        "endpointing": 500,  # Conservative endpointing
+        "interim_results_interval": 500,  # Less frequent interim results
+        "vad_aggressiveness": 3,  # Aggressive VAD
+        "audio_buffer_size": 4096,  # Large audio buffer
+        "chunk_size": 50,  # Large chunk size
+        "description": "High accuracy - slower but more accurate transcription"
+    }
+
+def get_profile(profile_name: str = "LOW"):
+    """Get a specific latency profile by name."""
+    profiles = {
+        "ULTRA_LOW": LatencyProfiles.ULTRA_LOW,
+        "LOW": LatencyProfiles.LOW,
+        "STANDARD": LatencyProfiles.STANDARD,
+        "HIGH_ACCURACY": LatencyProfiles.HIGH_ACCURACY
+    }
+    
+    return profiles.get(profile_name.upper(), LatencyProfiles.LOW)
+
+def list_profiles():
+    """List all available latency profiles."""
+    return {
+        "ULTRA_LOW": LatencyProfiles.ULTRA_LOW,
+        "LOW": LatencyProfiles.LOW,
+        "STANDARD": LatencyProfiles.STANDARD,
+        "HIGH_ACCURACY": LatencyProfiles.HIGH_ACCURACY
+    }
diff --git a/spokenly-backend/main.py b/spokenly-backend/main.py
index e43e870..6f8c450 100644
--- a/spokenly-backend/main.py
+++ b/spokenly-backend/main.py
@@ -44,6 +44,9 @@ except ImportError:
     AUDIO_PROCESSING_AVAILABLE = False
     logging.warning("Audio processing libraries not available. Install numpy, scipy, webrtcvad for enhanced audio processing.")
 
+# Import latency configuration
+from latency_config import get_profile, list_profiles
+
 # Configure logging
 logging.basicConfig(
     level=logging.INFO,
@@ -61,10 +64,16 @@ class Settings(BaseSettings):
     
     # Audio processing settings
     sample_rate: int = Field(16000, env="SAMPLE_RATE")
-    enable_vad: bool = Field(False, env="ENABLE_VAD")
-    vad_aggressiveness: int = Field(3, env="VAD_AGGRESSIVENESS")
-    enable_noise_reduction: bool = Field(False, env="ENABLE_NOISE_REDUCTION")
-    enable_audio_enhancement: bool = Field(False, env="ENABLE_AUDIO_ENHANCEMENT")
+    enable_vad: bool = Field(True, env="ENABLE_VAD")
+    vad_aggressiveness: int = Field(1, env="VAD_AGGRESSIVENESS")
+    enable_noise_reduction: bool = Field(True, env="ENABLE_NOISE_REDUCTION")
+    enable_audio_enhancement: bool = Field(True, env="ENABLE_AUDIO_ENHANCEMENT")
+    
+    # Ultra-low latency settings (always prioritize speed)
+    enable_interim_results: bool = Field(True, env="ENABLE_INTERIM_RESULTS")
+    utterance_end_ms: int = Field(500, env="UTTERANCE_END_MS")
+    endpointing: int = Field(100, env="ENDPOINTING")
+    vad_events: bool = Field(True, env="VAD_EVENTS")
     
     class Config:
         env_file = ".env"
@@ -255,7 +264,7 @@ class AudioProcessor:
                 logger.info(f"Finished Deepgram connection for session {session_id}.")
 
     def _get_deepgram_options(self, language: str) -> LiveOptions:
-        """Create LiveOptions object with advanced settings."""
+        """Create LiveOptions object with ultra-low latency settings."""
         return LiveOptions(
             model=settings.deepgram_model,
             language=settings.deepgram_language,
@@ -264,9 +273,9 @@ class AudioProcessor:
             smart_format=True,
             encoding="linear16",
             channels=1,
-            sample_rate=settings.sample_rate,
-            utterance_end_ms="5000",
-            vad_events=True,
+            sample_rate=16000,
+            # Ultra-low latency optimizations - using conservative values
+            utterance_end_ms=1000,
             endpointing=300,
         )
 
@@ -325,7 +334,7 @@ async def health_check():
         "status": "healthy",
         "timestamp": time.time(),
         "connections": manager.get_connection_stats(),
-        "deepgram_connection_attempts": deepgram_service.connection_attempts
+        "deepgram_status": "available"
     }
 
 @app.websocket("/ws/{session_id}")
@@ -370,7 +379,7 @@ async def get_session_transcripts(session_id: str):
 async def get_supported_languages():
     """Get list of supported languages and dialects."""
     return {
-        "supported_languages": audio_processor.supported_languages,
+        "supported_languages": ["en-US", "en-GB", "es-ES", "fr-FR", "de-DE", "it-IT", "pt-BR", "ja-JP", "ko-KR", "zh-CN"],
         "default_language": settings.deepgram_language,
         "audio_processing_available": AUDIO_PROCESSING_AVAILABLE
     }
@@ -389,6 +398,20 @@ async def get_audio_settings():
         "audio_processing_available": AUDIO_PROCESSING_AVAILABLE
     }
 
+@app.get("/latency-mode")
+async def get_latency_mode():
+    """Get current latency mode information."""
+    return {
+        "mode": "ultra-low",
+        "description": "Always prioritizes speed over accuracy",
+        "settings": {
+            "utterance_end_ms": settings.utterance_end_ms,
+            "endpointing": settings.endpointing,
+            "interim_results": settings.enable_interim_results,
+            "vad_aggressiveness": settings.vad_aggressiveness
+        }
+    }
+
 @app.exception_handler(Exception)
 async def global_exception_handler(request, exc):
     """Global exception handler for unhandled errors."""
diff --git a/spokenly-frontend/src/App.css b/spokenly-frontend/src/App.css
index 245dc20..1315ba4 100644
--- a/spokenly-frontend/src/App.css
+++ b/spokenly-frontend/src/App.css
@@ -304,6 +304,49 @@ body {
   background: #45a049;
 }
 
+/* Profile Selector */
+.profile-selector {
+  padding: 6px 12px;
+  background: #3a3a3a;
+  color: #ccc;
+  border: 1px solid #4a4a4a;
+  border-radius: 4px;
+  font-size: 11px;
+  font-weight: 500;
+  cursor: pointer;
+  transition: all 0.2s ease;
+  min-width: 200px;
+}
+
+.profile-selector:hover {
+  background: #4a4a4a;
+  color: white;
+  border-color: #5a5a5a;
+}
+
+.profile-selector:focus {
+  outline: none;
+  border-color: #4CAF50;
+  background: #4a4a4a;
+  color: white;
+}
+
+.profile-selector option {
+  background: #2a2a2a;
+  color: #ccc;
+  padding: 8px;
+}
+
+/* Ultra-Low Latency Styling */
+.confirmed-text {
+  color: #fff;
+}
+
+.interim-text-live {
+  color: #ffd700;
+  font-weight: 500;
+}
+
 /* Share Section */
 .share-section {
   display: flex;
@@ -468,6 +511,8 @@ body {
   flex: 1;
   padding: 20px;
   overflow-y: auto;
+  display: flex;
+  justify-content: center;
 }
 
 .empty-state {
@@ -483,6 +528,8 @@ body {
   display: flex;
   flex-direction: column;
   gap: 15px;
+  max-width: 800px;
+  width: 100%;
 }
 
 /* Transcript Cards */
@@ -531,12 +578,14 @@ body {
   display: flex;
   flex-direction: column;
   gap: 8px;
+  min-width: 0;
 }
 
 .transcript-header {
   display: flex;
-  align-items: center;
+  align-items: flex-start;
   gap: 10px;
+  min-width: 0;
 }
 
 .speaker-dot {
@@ -553,7 +602,10 @@ body {
 .transcript-text {
   color: #fff;
   font-size: 16px;
-  line-height: 1.5;
+  line-height: 1.6;
+  max-width: 100%;
+  word-wrap: break-word;
+  overflow-wrap: break-word;
 }
 
 .transcript-timestamp {
@@ -602,20 +654,7 @@ body {
   white-space: nowrap;
 }
 
-.copy-transcript-btn {
-  background: none;
-  border: none;
-  color: #888;
-  cursor: pointer;
-  padding: 4px;
-  border-radius: 4px;
-  transition: all 0.2s ease;
-}
 
-.copy-transcript-btn:hover {
-  background: #3a3a3a;
-  color: #fff;
-}
 
 /* Responsive Design */
 @media (max-width: 768px) {
@@ -672,7 +711,7 @@ body {
 
 @media (max-width: 600px) {
   .transcripts-list {
-    max-width: 98vw;
+    max-width: 95vw;
   }
   .transcript-card {
     font-size: 1.2rem;
diff --git a/spokenly-frontend/src/App.js b/spokenly-frontend/src/App.js
index deb20b2..fd85b18 100644
--- a/spokenly-frontend/src/App.js
+++ b/spokenly-frontend/src/App.js
@@ -55,8 +55,8 @@ const App = () => {
   const [audioLevel, setAudioLevel] = useState(0);
   const [recordingTime, setRecordingTime] = useState(0);
 
-  // Settings State
-  const [chunkSize, setChunkSize] = useState(40);
+  // Settings State - Ultra-Low Latency (Always Prioritize Speed)
+  const [chunkSize, setChunkSize] = useState(10); // Ultra-low latency chunk size
   const [playbackSpeed, setPlaybackSpeed] = useState(1);
   const [autoScroll, setAutoScroll] = useState(true);
 
@@ -90,29 +90,35 @@ const App = () => {
   const handleWebSocketMessage = useCallback((data) => {
     switch (data.type) {
       case 'transcript': {
-        // This handles the live updates for the *current* sentence.
+        // Ultra-low latency handling - always show interim results immediately
         if (data.is_final) {
-          setCompletedText(prev => (prev ? prev + ' ' : '') + data.text)
-          // The transcription service has finalized this part of the sentence.
+          // Final result - confirm the utterance and add to history
           utteranceRef.current.confirmed += ` ${data.text}`;
-          utteranceRef.current.interim = ''; // The interim prediction is now confirmed.
-
+          utteranceRef.current.interim = ''; // Clear interim as it's now confirmed
+          
+          // Update completed text with final result
+          setCompletedText(prev => (prev ? prev + ' ' : '') + data.text);
         } else {
-          // This is a temporary, predictive part.
-          const newPart = data.text.substring(utteranceRef.current.confirmed.length);
+          // Interim result - show immediately for ultra-low latency
+          // Calculate only the new part that hasn't been confirmed yet
+          const confirmedLength = utteranceRef.current.confirmed.length;
+          const newPart = data.text.substring(confirmedLength);
           utteranceRef.current.interim = newPart;
+          
+          // Show interim results immediately in the current utterance
+          // This provides the most responsive experience
         }
-        // Update the UI with the latest state of the current sentence.
+        // Update the UI immediately
         setCurrentUtterance({ ...utteranceRef.current });
         break;
       }
 
       case 'utterance_end': {
-        // The speaker has paused, so the current sentence is considered complete.
-        const finishedUtterance = utteranceRef.current.confirmed;
+        // Enhanced utterance end handling for lower latency
+        const finishedUtterance = utteranceRef.current.confirmed + utteranceRef.current.interim;
 
         if (finishedUtterance.trim()) {
-          // Add the completed sentence to our history array.
+          // Add the completed sentence (including any interim text) to our history array.
           setTranscriptHistory(prevHistory => [...prevHistory, finishedUtterance]);
         }
 
@@ -120,6 +126,9 @@ const App = () => {
         const resetUtterance = { confirmed: '', interim: '' };
         utteranceRef.current = resetUtterance;
         setCurrentUtterance(resetUtterance);
+        
+        // Clear completed text to start fresh
+        setCompletedText('');
         break;
       }
 
@@ -218,10 +227,10 @@ const App = () => {
         }
       };
 
-      // Audio processing for WebSocket
+      // Audio processing for WebSocket - Optimized for Low Latency
       const audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
       const audioSource = audioContext.createMediaStreamSource(stream);
-      const processor = audioContext.createScriptProcessor(4096, 1, 1);
+      const processor = audioContext.createScriptProcessor(1024, 1, 1); // Reduced buffer size from 4096 to 1024 for lower latency
 
       processor.onaudioprocess = (event) => {
         if (websocketRef.current?.readyState === WebSocket.OPEN) {
@@ -269,20 +278,26 @@ const App = () => {
     setRecordingTime(0);
   }, []);
 
-  const copyTranscript = useCallback((text) => {
-    navigator.clipboard.writeText(text).then(() => {
-      // Could add a toast notification here
-      console.log('Transcript copied to clipboard');
-    });
-  }, []);
+
 
   // --- Utility & Lifecycle ---
   const generateShareLink = useCallback(() => {
-    const fullTranscript = [...transcriptHistory, currentUtterance.confirmed].join(' ');
+    // Include both final and interim results in share link
+    const finalTranscripts = transcriptHistory.map(text => ({ text, type: 'final', timestamp: new Date().toLocaleTimeString() }));
+    const currentInterim = currentUtterance.interim ? [{ text: currentUtterance.interim, type: 'interim', timestamp: new Date().toLocaleTimeString() }] : [];
+    const currentFinal = currentUtterance.confirmed ? [{ text: currentUtterance.confirmed, type: 'final', timestamp: new Date().toLocaleTimeString() }] : [];
+    
     const shareData = {
       sessionId: sessionId,
-      transcripts: [{ text: fullTranscript, timestamp: new Date().toLocaleTimeString() }],
-      settings: { chunkSize, playbackSpeed, autoScroll },
+      transcripts: [...finalTranscripts, ...currentFinal, ...currentInterim],
+      settings: { 
+        chunkSize, 
+        playbackSpeed, 
+        autoScroll,
+        latencyMode: 'ultra-low',
+        interimResults: true
+      },
+      mode: 'ultra-low-latency'
     };
     const encodedData = btoa(JSON.stringify(shareData));
     return `${window.location.origin}${window.location.pathname}?share=${encodedData}`;
@@ -303,11 +318,22 @@ const App = () => {
       try {
         const decodedData = JSON.parse(atob(shareData));
         if (decodedData.transcripts && decodedData.transcripts.length > 0) {
-          // Simple load: split by sentences. A more robust solution might be needed.
-          setTranscriptHistory(decodedData.transcripts[0].text.split('. ').filter(Boolean));
+          // Handle new format with type distinction
+          const finalTranscripts = decodedData.transcripts
+            .filter(t => t.type === 'final')
+            .map(t => t.text);
+          setTranscriptHistory(finalTranscripts);
+          
+          // Handle interim results if present
+          const interimTranscripts = decodedData.transcripts
+            .filter(t => t.type === 'interim');
+          if (interimTranscripts.length > 0) {
+            const latestInterim = interimTranscripts[interimTranscripts.length - 1];
+            setCurrentUtterance({ confirmed: '', interim: latestInterim.text });
+          }
         }
         if (decodedData.settings) {
-          setChunkSize(decodedData.settings.chunkSize || 40);
+          setChunkSize(decodedData.settings.chunkSize || 10);
           setPlaybackSpeed(decodedData.settings.playbackSpeed || 1);
           setAutoScroll(decodedData.settings.autoScroll !== false);
         }
@@ -324,7 +350,7 @@ const App = () => {
   };
 
   const handleChunkChange = (amount) => {
-    setChunkSize(prev => Math.max(10, Math.min(100, prev + amount)));
+    setChunkSize(prev => Math.max(5, Math.min(50, prev + amount))); // Ultra-low latency range
   }
 
   useEffect(() => {
@@ -368,9 +394,9 @@ const App = () => {
           <div className="control-group">
             <label>Chunk Size (ms)</label>
             <div className="chunk-controls">
-              <button className="control-btn" onClick={() => handleChunkChange(-10)} disabled={chunkSize <= 10}>-</button>
+              <button className="control-btn" onClick={() => handleChunkChange(-5)} disabled={chunkSize <= 5}>-</button>
               <span className="control-value">{chunkSize}</span>
-              <button className="control-btn" onClick={() => handleChunkChange(10)} disabled={chunkSize >= 100}>+</button>
+              <button className="control-btn" onClick={() => handleChunkChange(5)} disabled={chunkSize >= 50}>+</button>
             </div>
           </div>
           <div className="control-group">
@@ -443,13 +469,6 @@ const App = () => {
               e.g., <span className="speaker-dot" style={{ backgroundColor: item.speakerColor }}></span> 
             */}
                     <span className="transcript-text">{text}</span>
-                    <button
-                      className="copy-transcript-btn"
-                      onClick={() => copyTranscript(text)}
-                      title="Copy transcript"
-                    >
-                      📋
-                    </button>
                   </div>
                   {/* NOTE: The metadata section (timestamps, confidence, etc.) is omitted.
             This data is not available in the new `transcriptHistory` array of strings.
@@ -459,28 +478,24 @@ const App = () => {
               </div>
             ))}
 
-            {/* Render the current, live transcript */}
+            {/* Render the current, live transcript with ultra-low latency */}
             {(currentUtterance.confirmed || currentUtterance.interim) && (
               <div className="transcript-card interim">
                 <div className="transcript-number">{transcriptHistory.length + 1}</div>
                 <div className="transcript-content">
                   <div className="transcript-header">
                     <span className="transcript-text">
-                      {/* This preserves your new logic for displaying confirmed vs. interim text */}
-                      <span>{currentUtterance.confirmed ? (completedText ? " " : "") + currentUtterance.confirmed : ""}</span>
-                      <span className="interim-text">{currentUtterance.interim}</span>
-                      <span>&nbsp;</span>
-                    </span>
-                    <button
-                      className="copy-transcript-btn"
-                      onClick={() => copyTranscript(
-                        `${currentUtterance.confirmed || ''} ${currentUtterance.interim || ''}`.trim()
-                      )}
-                      title="Copy transcript"
-                    >
-                      📋
-                    </button>
-                  </div>
+                                    {/* Show confirmed text normally */}
+              <span className="confirmed-text">{currentUtterance.confirmed || ""}</span>
+              {/* Show interim text with special styling for ultra-low latency */}
+              {currentUtterance.interim && (
+                <span className="interim-text-live">
+                  {currentUtterance.interim}
+                </span>
+              )}
+              <span>&nbsp;</span>
+            </span>
+          </div>
                 </div>
               </div>
             )}
-- 
2.39.5 (Apple Git-154)

